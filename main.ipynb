{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package benepar_en3 to\n",
      "[nltk_data]     C:\\Users\\cedch\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package benepar_en3 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import benepar\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import decimal\n",
    "from collections import Counter\n",
    "import re\n",
    "from lxml import etree\n",
    "\n",
    "import apted\n",
    "from apted import APTED\n",
    "from apted.helpers import Tree\n",
    "from itertools import combinations\n",
    "\n",
    "from math import comb\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "benepar.download('benepar_en3')\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "if spacy.__version__.startswith('2'):\n",
    "    nlp.add_pipe(benepar.BeneparComponent(\"benepar_en3\"))\n",
    "else:\n",
    "    nlp.add_pipe('benepar', config={'model': 'benepar_en3'})\n",
    "\n",
    "lemmatizer = spacy.load('en_core_web_sm', disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEXP TO XML\n",
    "def clean_xml(xml):\n",
    "    xml = re.sub('<(/?)[^a-zA-Z/][^>]*>', '<\\g<1>UNK>', xml) # invalid tokens labeled 'UNK'\n",
    "    return xml.replace(' ', '')\n",
    "\n",
    "def sexp_to_xml(sexp):\n",
    "    def apply_inner_re(s):\n",
    "        return re.sub('\\(([^ ]*) ([^\\)\\(]*)\\)', '<\\g<1>> \\g<2> </\\g<1>>', s)\n",
    "\n",
    "    xml = apply_inner_re(sexp)\n",
    "    while xml.startswith('('):\n",
    "        xml = apply_inner_re(xml)\n",
    "\n",
    "    with open('special_chars.txt') as f:\n",
    "        special_chars = dict([line.split() for line in f])\n",
    "\n",
    "    def key_to_re(s):\n",
    "        s = re.sub('(.*)([\\\\\\.\\+\\*\\?\\^\\$\\(\\)\\[\\]\\{\\}\\|])(.*)', '\\g<1>\\\\\\\\\\g<2>\\g<3>', s)\n",
    "        return '<(/?)' + s + '>'\n",
    "\n",
    "    for k, v in special_chars.items():\n",
    "        xml = re.sub(key_to_re(k), f'<\\g<1>{v}>', xml)\n",
    "\n",
    "    return clean_xml(xml)\n",
    "\n",
    "# TREE EDIT DISTANCE\n",
    "def apted_format(parse_str):\n",
    "    parse_str = re.sub('\\(([^ ]+) [^ \\(\\)]+?\\)', '(\\g<1>)', parse_str)\n",
    "    parse_str = parse_str.replace(' ', '')\n",
    "    parse_str = parse_str.replace('(', '{')\n",
    "    parse_str = parse_str.replace(')', '}')\n",
    "    return parse_str\n",
    "\n",
    "# PARATACTIC CHILDREN STRICT \n",
    "def find_parataxis_strict(e):\n",
    "    global parataxis_clause_tags\n",
    "    print(e.getchildren())\n",
    "    children = [c.tag for c in e.getchildren() if not(c.tag.startswith('PUNCT-'))] # excludes punct\n",
    "    sum = 0\n",
    "    in_group = False\n",
    "    print(children)\n",
    "    \n",
    "    for i in range(len(children) - 1):\n",
    "        if children[i] in parataxis_clause_tags and children[i + 1] in parataxis_clause_tags:\n",
    "            sum += 1\n",
    "            if not(in_group):\n",
    "                sum += 1\n",
    "                in_group = True\n",
    "        else:\n",
    "            in_group = False\n",
    "    return sum if sum != 0 else 1\n",
    "\n",
    "# LEMMATIZATION\n",
    "def lemmatize(word: str):\n",
    "    return lemmatizer(word)[0].lemma_\n",
    "\n",
    "# AOA\n",
    "def aoa_of(word: str):\n",
    "    global aoa_df, aoa_mode\n",
    "    \n",
    "    search = aoa_df[aoa_df['Word'] == word]['Rating.Mean']\n",
    "    if len(search) == 1:\n",
    "        return float(search)\n",
    "    \n",
    "    lemma_search = aoa_df[aoa_df['Word'] == lemmatize(word)]['Rating.Mean']\n",
    "    if len(lemma_search) == 1:\n",
    "        return float(lemma_search)\n",
    "    \n",
    "    lemmas = aoa_df[aoa_df['Lemma'] == lemmatize(word)]['Rating.Mean']\n",
    "    if len(lemmas) == 0:\n",
    "        return -1\n",
    "    elif len(lemmas) == 1:\n",
    "        return float(lemmas)\n",
    "    elif aoa_mode == 'avg':\n",
    "        return sum(lemmas) / len(lemmas)\n",
    "    elif aoa_mode == 'max':\n",
    "        return max(lemmas)\n",
    "    \n",
    "    return -1\n",
    "\n",
    "#Average\n",
    "def list_avg(l):\n",
    "    return sum(l) / len(l) if len(l) > 0 else 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "clause_tags = ['S', 'SBARQ', 'SINV'] # Not included: 'SQ', 'SBAR'\n",
    "parataxis_clause_tags = ['S', 'SBARQ', 'SINV', 'SQ', 'SBAR']\n",
    "clause_re = re.compile('(' + '/|'.join(clause_tags) + ')')\n",
    "pronoun_tags = ['PRP', 'PRPS']\n",
    "\n",
    "aoa_df = pd.read_csv('reference/aoa/aoa_lemmas.csv')\n",
    "with open('reference/word_frequency/subtlexus_lower.json', encoding='utf-8') as f:\n",
    "    wf_dict = json.load(f)\n",
    "\n",
    "def benepar_analysis(sent):\n",
    "    features_lst = ['num_clauses', 'num_sbar', 'num_unk', 'depth_sum', 'clause_depth_sum', 'pronoun_sum', 'num_leaf_nps', 'num_nps', \n",
    "        'np_leaf_sum', 'clause_length_sum', 'paratactic_sum', 'root_parataxis_strict', \n",
    "        'root_parataxis_loose']\n",
    "    features = Counter({x : 0 for x in features_lst}) # initialize to 0\n",
    "    max_features = {\n",
    "        'max_clause_depth' : 0,\n",
    "        'max_depth' : 0, \n",
    "    }\n",
    "\n",
    "    # global num_clauses, num_sbar, num_unk, depth_sum, max_depth, max_clause_depth, clause_depth_sum\n",
    "    # global pronoun_sum, num_leaf_nps, num_nps, np_leaf_sum, clause_length_sum\n",
    "    # global paratactic_sum, root_parataxis_strict, root_parataxis_loose\n",
    "    paratactic_sum_local = 0\n",
    "\n",
    "    xml = sexp_to_xml(sent._.parse_string)\n",
    "    root = etree.fromstring(xml) \n",
    "    tree = etree.ElementTree(root)\n",
    "\n",
    "    features['root_parataxis_loose'] = max(1, sum(int(bool(c.tag in parataxis_clause_tags)) \n",
    "                                                                    for c in root.getchildren()))\n",
    "    features['root_parataxis_strict'] = find_parataxis_strict(root)\n",
    "    features['num_sbar'] = sum(int(e.tag == 'SBAR') for e in root.iter())\n",
    "    features['pronoun_sum'] = sum(int(e.tag in pronoun_tags) for e in root.iter())\n",
    "    features['num_unk'] = sum(int(e.tag == 'UNK') for e in root.iter())\n",
    "    \n",
    "    for e in root.iter():\n",
    "        tag = e.tag\n",
    "        if tag in clause_tags: \n",
    "            features['num_clauses'] += 1\n",
    "            features['clause_length_sum'] += sum(int(not(d.tag.startswith('PUNCT-') and bool(d.text))) for d in e.iterdescendants())\n",
    "            \n",
    "            paratactic_sum_local += sum(int(bool(c.tag in parataxis_clause_tags)) for c in e.getchildren())\n",
    "        elif e.tag == 'NP':\n",
    "            features['num_nps'] += 1\n",
    "            is_leaf_np = True\n",
    "            for c in e.iterdescendants():\n",
    "                if c.text: \n",
    "                    if not(c.tag.startswith('PUNCT-') or c.tag == 'DT'): # ignore determiners and punctuation\n",
    "                        features['np_leaf_sum'] += 1\n",
    "                else:\n",
    "                    is_leaf_np = False\n",
    "            if is_leaf_np:\n",
    "                features['num_leaf_nps'] += 1\n",
    "        \n",
    "        if e.text:\n",
    "            path = tree.getpath(e)\n",
    "\n",
    "            depth = len(re.findall('/', path))\n",
    "            features['depth_sum'] += depth # Number of times '/' appears, excluding first\n",
    "            max_features['max_depth'] = max(max_features['max_depth'], depth)\n",
    "\n",
    "            clause_depth = len(clause_re.findall(path))\n",
    "            features['clause_depth_sum'] += clause_depth\n",
    "            max_features['max_clause_depth'] = max(max_features['max_clause_depth'], clause_depth)\n",
    "            \n",
    "    features['paratactic_sum'] = max(1, paratactic_sum_local)\n",
    "\n",
    "    return features, max_features\n",
    "\n",
    "def spacy_analysis(sent, uniq_words: set):\n",
    "    features_lst = ['dep_dist_sum', 'num_words', 'words_before_root_sum', 'uniq_words', \n",
    "        'num_words_no_nums', 'num_stop_words', 'aoa_sum', 'aoa_count', \n",
    "        'aoa_stopless_sum', 'aoa_stopless_count', 'aoa_uniq_sum', 'aoa_uniq_count',\n",
    "        'aoa_stopless_uniq_sum', 'aoa_stopless_uniq_count', 'wf_sum', 'wf_count', \n",
    "        'wf_stopless_sum', 'wf_stopless_count', 'wf_uniq_sum', 'wf_uniq_count',\n",
    "        'wf_stopless_uniq_sum', 'wf_stopless_uniq_count']\n",
    "    features = Counter({x : 0 for x in features_lst})\n",
    "    max_features = {\n",
    "        'max_dep_dist': 0, \n",
    "    }\n",
    "\n",
    "    # global dep_dist_sum, num_words, words_before_root_sum, uniq_words, num_words_no_nums, num_stop_words\n",
    "    # global max_dep_dist\n",
    "    # global aoa_list, aoa_stopless_list, aoa_uniq_list, aoa_stopless_uniq_list\n",
    "    # global wf_list, wf_stopless_list, wf_uniq_list, wf_stopless_uniq_list\n",
    "\n",
    "    features['num_stop_words'] = sum(int(token.is_stop) for token in sent)\n",
    "\n",
    "    for token in sent:\n",
    "        if not(token.is_punct or token.is_space):\n",
    "            features['num_words'] += 1\n",
    "            dep_dist = abs(token.head.i - token.i)\n",
    "            features['dep_dist_sum'] += dep_dist\n",
    "            max_features['max_dep_dist'] = max(dep_dist, max_features['max_dep_dist'])\n",
    "            \n",
    "            if (wf := wf_dict.get(token.lower_)) is not None:\n",
    "                features['wf_sum'] += wf\n",
    "                features['wf_count'] += 1\n",
    "                # wf_list.append(wf)\n",
    "                if not token.is_stop:\n",
    "                    features['wf_stopless_sum'] += wf\n",
    "                    features['wf_stopless_count'] += 1\n",
    "                    # wf_stopless_list.append(wf)\n",
    "\n",
    "            if (aoa := aoa_of(token.lower_)) != -1:\n",
    "                features['aoa_sum'] += aoa\n",
    "                features['aoa_count'] += 1\n",
    "                # aoa_list.append(aoa)\n",
    "                if not token.is_stop:\n",
    "                    features['aoa_stopless_sum'] += aoa\n",
    "                    features['aoa_stopless_count'] += 1\n",
    "                    # aoa_stopless_list.append(aoa)\n",
    "\n",
    "            if token.i < sent.root.i: \n",
    "                features['words_before_root_sum'] += 1\n",
    "\n",
    "            if not(token.like_num):\n",
    "                if not(token.lower_ in uniq_words):\n",
    "                    uniq_words.add(token.lower_)\n",
    "\n",
    "                    if wf is not None:\n",
    "                        features['wf_uniq_sum'] += wf\n",
    "                        features['wf_uniq_count'] += 1\n",
    "                        # wf_uniq_list.append(wf)\n",
    "                        if not token.is_stop:\n",
    "                            features['wf_stopless_uniq_sum'] += wf\n",
    "                            features['wf_stopless_uniq_count'] += 1\n",
    "                            # wf_stopless_uniq_list.append(wf)\n",
    "                    \n",
    "                    if aoa != -1:\n",
    "                        features['aoa_uniq_sum'] += aoa\n",
    "                        features['aoa_uniq_count'] += 1\n",
    "                        # aoa_uniq_list.append(aoa)\n",
    "                        if not token.is_stop:\n",
    "                            features['aoa_stopless_uniq_sum'] += aoa\n",
    "                            features['aoa_stopless_uniq_count'] += 1\n",
    "                            # aoa_stopless_uniq_list.append(aoa)\n",
    "\n",
    "                features['num_words_no_nums'] += 1\n",
    "    return features, max_features, uniq_words\n",
    "    \n",
    "def ted_analysis(sent1, sent2):\n",
    "    global ted_sum\n",
    "\n",
    "    tree1 = Tree.from_text(apted_format(sent1._.parse_string))\n",
    "    tree2 = Tree.from_text(apted_format(sent2._.parse_string))\n",
    "\n",
    "    apted = APTED(tree1, tree2, )\n",
    "    ted = apted.compute_edit_distance()\n",
    "    ted_sum += ted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(NP (DT this) (SBAR (IN that) (S (NP (PRP he)) (VP (MD will) (VP (VB go))))) (: :) (SBAR (IN that) (S (NP (PRP i)) (VP (VBP am) (ADJP (JJ clear))))))\n",
    "(NP (NP (DT this)) (SBAR (WHNP (WDT that)) (S (NP (PRP he)) (VP (MD will) (VP (VB go)))))) (: :) (SBAR (IN that) (S (NP (PRP i)) (VP (VBP am) (ADJP (JJ clear)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cedch\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\distributions\\distribution.py:45: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
      "  warnings.warn(f'{self.__class__} does not define `arg_constraints`. ' +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Element NP at 0x2b1da999640>, <Element PUNCT-COLON at 0x2b1da9a7000>, <Element S at 0x2b1c4535400>, <Element PUNCT-PERIOD at 0x2b1d5aa7b40>]\n",
      "['NP', 'S']\n",
      "[<Element PP at 0x2b1d48a7080>, <Element PUNCT-COMMA at 0x2b1da1692c0>, <Element NP at 0x2b1d5a71f80>, <Element VP at 0x2b1c4534d80>, <Element PUNCT-PERIOD at 0x2b1de1df5c0>]\n",
      "['PP', 'NP', 'VP']\n",
      "[<Element PP at 0x2b20d58e880>, <Element PUNCT-COMMA at 0x2b20d58f9c0>, <Element NP at 0x2b20d58de00>, <Element VP at 0x2b20d58d000>, <Element PUNCT-PERIOD at 0x2b20d58c040>]\n",
      "['PP', 'NP', 'VP']\n",
      "[<Element PP at 0x2b20d58d740>, <Element NP at 0x2b1da2d8500>, <Element VP at 0x2b1b5dc84c0>, <Element PUNCT-PERIOD at 0x2b1b5dcb440>]\n",
      "['PP', 'NP', 'VP']\n",
      "[<Element NP at 0x2b1dead4980>, <Element VP at 0x2b1dead5740>, <Element PUNCT-PERIOD at 0x2b1dead78c0>]\n",
      "['NP', 'VP']\n",
      "[<Element PP at 0x2b1dc116b80>, <Element PUNCT-COMMA at 0x2b2160f1100>, <Element NP at 0x2b1de0e14c0>, <Element VP at 0x2b1de0e06c0>, <Element PUNCT-PERIOD at 0x2b1de0e1d00>]\n",
      "['PP', 'NP', 'VP']\n",
      "[<Element PP at 0x2b1de0e2580>, <Element PUNCT-COMMA at 0x2b1de0e3700>, <Element NP at 0x2b1de0e04c0>, <Element VP at 0x2b1de0e2380>, <Element PUNCT-PERIOD at 0x2b1de0e3300>]\n",
      "['PP', 'NP', 'VP']\n",
      "[<Element NP at 0x2b1da0bf5c0>, <Element VP at 0x2b1da0be840>, <Element PUNCT-PERIOD at 0x2b1da0bf400>]\n",
      "['NP', 'VP']\n",
      "[<Element S at 0x2b1de931300>, <Element PUNCT-COLON at 0x2b1de933400>, <Element CC at 0x2b1de930540>, <Element S at 0x2b1de930d80>, <Element PUNCT-PERIOD at 0x2b1de933e00>]\n",
      "['S', 'CC', 'S']\n",
      "[<Element NP at 0x2b1da157880>, <Element VP at 0x2b1da156d80>, <Element PUNCT-PERIOD at 0x2b1da156980>]\n",
      "['NP', 'VP']\n",
      "[<Element NP at 0x2b20d5c6d00>, <Element VP at 0x2b20d5c73c0>, <Element PUNCT-PERIOD at 0x2b20d5c75c0>]\n",
      "['NP', 'VP']\n",
      "[<Element PP at 0x2b1da1579c0>, <Element NP at 0x2b1da157980>, <Element VP at 0x2b22f1cfd00>, <Element PUNCT-PERIOD at 0x2b1da1d7b00>, <Element UNK at 0x2b1da1d4800>]\n",
      "['PP', 'NP', 'VP', 'UNK']\n",
      "[<Element NP at 0x2b1da1d7e40>, <Element VP at 0x2b1da1d4800>, <Element PUNCT-PERIOD at 0x2b1da1d7a80>]\n",
      "['NP', 'VP']\n",
      "[<Element NP at 0x2b1ae49b700>, <Element VP at 0x2b1da1d4680>, <Element PUNCT-PERIOD at 0x2b1da1d49c0>]\n",
      "['NP', 'VP']\n",
      "[<Element PP at 0x2b1da1d7dc0>, <Element NP at 0x2b1da1cf5c0>, <Element VP at 0x2b1da1cdc40>, <Element PUNCT-PERIOD at 0x2b1da1ce840>]\n",
      "['PP', 'NP', 'VP']\n",
      "[<Element NP at 0x2b1ae188440>, <Element VP at 0x2b1ae189400>, <Element PUNCT-PERIOD at 0x2b1ae18b1c0>]\n",
      "['NP', 'VP']\n",
      "[<Element PP at 0x2b1da19fb40>, <Element PUNCT-COMMA at 0x2b1da19ffc0>, <Element NP at 0x2b1da19f240>, <Element VP at 0x2b1da19c940>, <Element PUNCT-PERIOD at 0x2b1da19cbc0>]\n",
      "['PP', 'NP', 'VP']\n",
      "[<Element S at 0x2b1da19c1c0>, <Element PUNCT-COLON at 0x2b1da19f540>, <Element IN at 0x2b1da19ce80>, <Element S at 0x2b1da19df40>, <Element PUNCT-PERIOD at 0x2b1da19f980>]\n",
      "['S', 'IN', 'S']\n",
      "[<Element PP at 0x2b1da19e380>, <Element NP at 0x2b1da19c140>, <Element VP at 0x2b1da19ea00>, <Element PUNCT-PERIOD at 0x2b1da19ee40>]\n",
      "['PP', 'NP', 'VP']\n",
      "[<Element NP at 0x2b1da125a40>, <Element VP at 0x2b1da125cc0>, <Element PUNCT-PERIOD at 0x2b1da1271c0>]\n",
      "['NP', 'VP']\n",
      "[<Element SBAR at 0x2b1da125540>, <Element PUNCT-COMMA at 0x2b1da125940>, <Element NP at 0x2b1da126800>, <Element VP at 0x2b1da126280>, <Element PUNCT-PERIOD at 0x2b1da126b00>]\n",
      "['SBAR', 'NP', 'VP']\n",
      "[<Element S at 0x2b1da126dc0>, <Element PUNCT-COLON at 0x2b1da124700>, <Element CC at 0x2b1da127940>, <Element S at 0x2b1da124380>, <Element PUNCT-PERIOD at 0x2b1da1276c0>]\n",
      "['S', 'CC', 'S']\n",
      "[<Element S at 0x2b1da126440>, <Element PUNCT-COLON at 0x2b1da124940>, <Element CC at 0x2b1da126180>, <Element S at 0x2b1da126e00>, <Element PUNCT-PERIOD at 0x2b1dc14ed80>]\n",
      "['S', 'CC', 'S']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cedch\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\distributions\\distribution.py:45: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
      "  warnings.warn(f'{self.__class__} does not define `arg_constraints`. ' +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Element NP at 0x2b1da1da1c0>, <Element PUNCT-COLON at 0x2b1d5acab00>, <Element S at 0x2b1d5aa41c0>, <Element PUNCT-PERIOD at 0x2b1de9e2dc0>]\n",
      "['NP', 'S']\n",
      "[<Element SBAR at 0x2b1b5de2cc0>, <Element PUNCT-COMMA at 0x2b1b5de1200>, <Element NP at 0x2b1de2dfc80>, <Element VP at 0x2b1daba4c00>, <Element PUNCT-PERIOD at 0x2b1daba4e00>]\n",
      "['SBAR', 'NP', 'VP']\n",
      "[<Element PP at 0x2b1da972040>, <Element NP at 0x2b1da972300>, <Element VP at 0x2b1da971d40>, <Element PUNCT-PERIOD at 0x2b1da972f40>]\n",
      "['PP', 'NP', 'VP']\n",
      "[<Element NP at 0x2b1dc15c3c0>, <Element PUNCT-COLON at 0x2b1d5aa5dc0>, <Element SBAR at 0x2b1d5aa6780>, <Element PUNCT-PERIOD at 0x2b1d5aa7080>]\n",
      "['NP', 'SBAR']\n",
      "Error Log:\n",
      "\n",
      "                                              1789_Washington_Inaugural_Address  \\\n",
      "date                                                                 1789-04-30   \n",
      "pres_name                                                     George Washington   \n",
      "byline                          1st President of the United States: 1789 ‐ 1797   \n",
      "title                                                         Inaugural Address   \n",
      "benepar_analysis_time                                                       0.0   \n",
      "spacy_analysis_time                                                        0.23   \n",
      "tree_edit_distance_time                                                    6.09   \n",
      "total_file_analysis_time                                                  11.67   \n",
      "num_tokens                                                                 1546   \n",
      "num_sentences                                                                23   \n",
      "avg_tree_edit_dist_adjacent                                              138.23   \n",
      "avg_node_depth                                                            12.68   \n",
      "max_node_depth                                                               50   \n",
      "avg_node_clause_depth                                                      2.46   \n",
      "max_node_clause_depth                                                         8   \n",
      "avg_clause_length                                                         55.32   \n",
      "clauses_per_sent                                                           6.26   \n",
      "sbars_per_sent                                                              3.3   \n",
      "pronouns_per_sent                                                           4.7   \n",
      "pronouns_per_clause                                                        0.75   \n",
      "pronoun_prop_of_leaf_nps                                                   0.31   \n",
      "avg_num_np_modifiers                                                        5.1   \n",
      "loose_parataxis_per_sent                                                   1.48   \n",
      "root_parataxis_per_sent_strict                                              1.0   \n",
      "root_parataxis_per_sent_loose                                              1.17   \n",
      "num_unk                                                                       2   \n",
      "num_words                                                                  1434   \n",
      "avg_dependency_distance                                                    3.56   \n",
      "max_dependency_distance                                                      82   \n",
      "avg_sentence_length_by_tok                                                67.22   \n",
      "avg_sentence_length_by_word                                               62.35   \n",
      "avg_words_before_root                                                     13.48   \n",
      "num_uniq_words                                                              593   \n",
      "proportion_uniq                                                            0.42   \n",
      "stop_words_per_clause                                                      5.92   \n",
      "stop_words_per_sentence                                                   37.09   \n",
      "avg_aoa_max                                                                 6.2   \n",
      "avg_aoa_uniq_max                                                           8.12   \n",
      "avg_stopless_aoa_max                                                       8.62   \n",
      "avg_stopless_aoa_uniq_max                                                  8.84   \n",
      "avg_word_freq                                                           6791.01   \n",
      "avg_word_freq_uniq                                                        835.1   \n",
      "avg_word_freq_stopless                                                    94.66   \n",
      "avg_word_freq_stopless_uniq                                               76.64   \n",
      "\n",
      "                                              1793_Washington_Inaugural_Address  \n",
      "date                                                                 1793-03-04  \n",
      "pres_name                                                     George Washington  \n",
      "byline                          1st President of the United States: 1789 ‐ 1797  \n",
      "title                                                         Inaugural Address  \n",
      "benepar_analysis_time                                                       0.0  \n",
      "spacy_analysis_time                                                        0.15  \n",
      "tree_edit_distance_time                                                    0.14  \n",
      "total_file_analysis_time                                                   0.72  \n",
      "num_tokens                                                                  147  \n",
      "num_sentences                                                                 4  \n",
      "avg_tree_edit_dist_adjacent                                                79.0  \n",
      "avg_node_depth                                                             9.06  \n",
      "max_node_depth                                                               23  \n",
      "avg_node_clause_depth                                                      1.66  \n",
      "max_node_clause_depth                                                         4  \n",
      "avg_clause_length                                                          30.8  \n",
      "clauses_per_sent                                                           3.75  \n",
      "sbars_per_sent                                                             2.25  \n",
      "pronouns_per_sent                                                          3.25  \n",
      "pronouns_per_clause                                                        0.87  \n",
      "pronoun_prop_of_leaf_nps                                                   0.35  \n",
      "avg_num_np_modifiers                                                       4.11  \n",
      "loose_parataxis_per_sent                                                   1.25  \n",
      "root_parataxis_per_sent_strict                                              1.0  \n",
      "root_parataxis_per_sent_loose                                               1.0  \n",
      "num_unk                                                                       0  \n",
      "num_words                                                                   135  \n",
      "avg_dependency_distance                                                    3.07  \n",
      "max_dependency_distance                                                      46  \n",
      "avg_sentence_length_by_tok                                                36.75  \n",
      "avg_sentence_length_by_word                                               33.75  \n",
      "avg_words_before_root                                                      16.5  \n",
      "num_uniq_words                                                               90  \n",
      "proportion_uniq                                                            0.67  \n",
      "stop_words_per_clause                                                      5.27  \n",
      "stop_words_per_sentence                                                   19.75  \n",
      "avg_aoa_max                                                                6.05  \n",
      "avg_aoa_uniq_max                                                           6.92  \n",
      "avg_stopless_aoa_max                                                       8.58  \n",
      "avg_stopless_aoa_uniq_max                                                  8.52  \n",
      "avg_word_freq                                                           8143.49  \n",
      "avg_word_freq_uniq                                                      2832.02  \n",
      "avg_word_freq_stopless                                                     78.1  \n",
      "avg_word_freq_stopless_uniq                                                75.3  \n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame([])\n",
    "\n",
    "# TED Mode\n",
    "#  - 'combinations': Averages the TED of every pair of sentences in a doc\n",
    "#  - 'adjacent': Averages the TED of a sentence and the next sentence in a doc\n",
    "ted_mode = 'adjacent'\n",
    "# AoA Mode: If word is not in AoA list and multiple lemma matches exist\n",
    "#  - 'max': Chooses max of lemma matches\n",
    "#  - 'avg': Averages lemma matches\n",
    "aoa_mode = 'max'\n",
    "log = \"\"\n",
    "\n",
    "for i, file in enumerate(os.scandir('data/text_jsons/')):\n",
    "    if i == 2: break\n",
    "    \n",
    "    file_time = time.perf_counter()\n",
    "    file_name = re.sub('\\.json$', '', file.name)\n",
    "    with open(file, encoding='utf-8') as f:\n",
    "        metadata = json.load(f)\n",
    "    text = metadata['text']\n",
    "    text = text.replace('\\n', ' ').strip()\n",
    "    text = re.sub('\\s{2,}', ' ', text)\n",
    "    \n",
    "    try:\n",
    "        doc = nlp(text)\n",
    "    except ValueError:\n",
    "        error_text = f\"ValueError in '{file_name}'. Likely exists too long sentence. Skipping.\"\n",
    "        log += error_text + '\\n'\n",
    "        continue\n",
    "    except:\n",
    "        error_text = f\"Some other error occured in '{file_name}'. Skipping.\"\n",
    "        log += error_text + '\\n'\n",
    "        continue\n",
    "\n",
    "    sents = list(doc.sents)\n",
    "\n",
    "    # Doc-level\n",
    "    num_tokens = len(doc)\n",
    "    num_sents = len(sents)\n",
    "    ted_sum = 0\n",
    "\n",
    "    features_lst = ['num_clauses', 'num_sbar', 'num_unk', 'depth_sum', 'max_depth', \n",
    "        'max_clause_depth', 'clause_depth_sum', 'pronoun_sum', 'num_leaf_nps', 'num_nps', \n",
    "        'np_leaf_sum', 'clause_length_sum', 'paratactic_sum', 'root_parataxis_strict', \n",
    "        'root_parataxis_loose', 'dep_dist_sum', 'num_words', 'words_before_root_sum', 'uniq_words', \n",
    "        'num_words_no_nums', 'num_stop_words', 'max_dep_dist', 'aoa_sum', 'aoa_count', \n",
    "        'aoa_stopless_sum', 'aoa_stopless_count', 'aoa_uniq_sum', 'aoa_uniq_count',\n",
    "        'aoa_stopless_uniq_sum', 'aoa_stopless_uniq_count', 'wf_sum', 'wf_count', \n",
    "        'wf_stopless_sum', 'wf_stopless_count', 'wf_uniq_sum', 'wf_uniq_count',\n",
    "        'wf_stopless_uniq_sum', 'wf_stopless_uniq_count']\n",
    "    features = Counter({x : 0 for x in features_lst})\n",
    "    uniq_words = set()\n",
    "\n",
    "    for sent in sents:\n",
    "        benepar_time = time.perf_counter()\n",
    "        b_features, b_max_features = benepar_analysis(sent)\n",
    "        features.update(b_features)\n",
    "        for feature in b_max_features:\n",
    "            features[feature] = max(features.get(feature), b_max_features[feature])\n",
    "        benepar_time = time.perf_counter() - benepar_time\n",
    "\n",
    "        spacy_time = time.perf_counter()\n",
    "        s_features, s_max_features, sent_vocab = spacy_analysis(sent, uniq_words)\n",
    "        features.update(s_features)\n",
    "        for feature in s_max_features:\n",
    "            features[feature] = max(features.get(feature), s_max_features[feature])\n",
    "        uniq_words.update(sent_vocab)\n",
    "        spacy_time = time.perf_counter() - spacy_time\n",
    "\n",
    "    # TREE EDIT DISTANCE\n",
    "    ted_time = time.perf_counter()\n",
    "    if ted_mode == 'adjacent':    \n",
    "        for i in range(num_sents - 1):\n",
    "            ted_analysis(sents[i], sents[i + 1])\n",
    "        ted_avg = ted_sum / (num_sents - 1)\n",
    "    elif ted_mode == 'combinations':\n",
    "        for sent1, sent2 in combinations(sents, 2):\n",
    "            ted_analysis(sent1, sent2)\n",
    "        ted_avg = ted_sum / comb(num_sents, 2)\n",
    "    else:\n",
    "        print('Invalid ted_mode:', ted_mode)\n",
    "        ted_avg = -1\n",
    "    ted_time = time.perf_counter() - ted_time\n",
    "\n",
    "    summary = {\n",
    "        # File-level\n",
    "        'date' : metadata['date'],\n",
    "        'pres_name' : metadata['pres_name'],\n",
    "        'byline' : metadata['byline'],\n",
    "        'title' : metadata['title'],\n",
    "\n",
    "        # Performance time\n",
    "        'benepar_analysis_time' : benepar_time,\n",
    "        'spacy_analysis_time' : spacy_time,\n",
    "        'tree_edit_distance_time' : ted_time,\n",
    "        'total_file_analysis_time' : time.perf_counter() - file_time,\n",
    "\n",
    "        # Doc-level\n",
    "        'num_tokens' : num_tokens,\n",
    "        'num_sentences' : num_sents, \n",
    "        # 'avg_ted_adj' : ted_avg_adj,\n",
    "        # 'avg_ted_comb' : ted_avg_comb,\n",
    "        f'avg_tree_edit_dist_{ted_mode}' : ted_avg,\n",
    "\n",
    "        # Benepar\n",
    "        'avg_node_depth' : features['depth_sum'] / num_tokens, \n",
    "        'max_node_depth' : features['max_depth'], # Equivalent to tree height\n",
    "        'avg_node_clause_depth' : features['clause_depth_sum'] / num_tokens,\n",
    "        'max_node_clause_depth' : features['max_clause_depth'],\n",
    "        'avg_clause_length' : features['clause_length_sum'] / features['num_clauses'],\n",
    "        'clauses_per_sent' : features['num_clauses'] / num_sents, \n",
    "        'sbars_per_sent' : features['num_sbar'] / num_sents,\n",
    "        'pronouns_per_sent' : features['pronoun_sum'] / num_sents,\n",
    "        'pronouns_per_clause' : features['pronoun_sum'] / features['num_clauses'],\n",
    "        'pronoun_prop_of_leaf_nps' : features['pronoun_sum'] / features['num_leaf_nps'],\n",
    "        'avg_num_np_modifiers' : features['np_leaf_sum'] / features['num_nps'],\n",
    "        'loose_parataxis_per_sent' : features['paratactic_sum'] / num_sents,\n",
    "        'root_parataxis_per_sent_strict' : features['root_parataxis_strict'] / num_sents,\n",
    "        'root_parataxis_per_sent_loose' : features['root_parataxis_loose'] / num_sents,\n",
    "        'num_unk' : features['num_unk'],\n",
    "\n",
    "        # spaCy\n",
    "        'num_words' : features['num_words'],\n",
    "        'avg_dependency_distance' : features['dep_dist_sum'] / features['num_words'],\n",
    "        'max_dependency_distance' : features['max_dep_dist'],\n",
    "        'avg_sentence_length_by_tok' : num_tokens / num_sents, \n",
    "        'avg_sentence_length_by_word' : features['num_words'] / num_sents,\n",
    "        'avg_words_before_root' : features['words_before_root_sum'] / num_sents,\n",
    "        'num_uniq_words' : len(uniq_words), \n",
    "        'proportion_uniq' : len(uniq_words) / features['num_words_no_nums'],\n",
    "        'stop_words_per_clause' : features['num_stop_words'] / features['num_clauses'],\n",
    "        'stop_words_per_sentence' : features['num_stop_words'] / num_sents,\n",
    "        f'avg_aoa_{aoa_mode}' : features['aoa_sum'] / features['aoa_count'],\n",
    "        f'avg_aoa_uniq_{aoa_mode}' : features['aoa_uniq_sum'] / features['aoa_uniq_count'],\n",
    "        f'avg_stopless_aoa_{aoa_mode}' : features['aoa_stopless_sum'] / features['aoa_stopless_count'],\n",
    "        f'avg_stopless_aoa_uniq_{aoa_mode}' : features['aoa_stopless_uniq_sum'] / features['aoa_stopless_uniq_count'],\n",
    "        'avg_word_freq' : features['wf_sum'] / features['wf_count'],\n",
    "        'avg_word_freq_uniq' : features['wf_uniq_sum'] / features['wf_uniq_count'],\n",
    "        'avg_word_freq_stopless' : features['wf_stopless_sum'] / features['wf_stopless_count'],\n",
    "        'avg_word_freq_stopless_uniq' : features['wf_stopless_uniq_sum'] / features['wf_stopless_uniq_count'],\n",
    "    }      \n",
    "    \n",
    "    results[file_name] = summary\n",
    "    \n",
    "print(\"Error Log:\")\n",
    "print(log)\n",
    "pd.set_option('display.precision', 2)\n",
    "print(results)\n",
    "results.to_csv(f\"results/{datetime.now().strftime('%m-%d-%Y_%H-%M')}.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.798829799226471   0.13868336305891654 0.03236415372895239]\n",
      "0.96987731601434\n",
      "                                1         2         3\n",
      "avg_word_freq_stopless       0.84  1.72e-01 -4.53e-01\n",
      "avg_word_freq_stopless_uniq  0.43  4.56e-02  8.78e-01\n",
      "avg_tree_edit_dist_adjacent -0.18  2.06e-01  6.46e-02\n",
      "max_dependency_distance     -0.17  8.83e-01 -2.74e-02\n",
      "avg_sentence_length_by_tok  -0.10  1.34e-02 -1.67e-03\n",
      "avg_sentence_length_by_word -0.10  3.90e-03 -2.90e-03\n",
      "avg_clause_length           -0.08  7.60e-03 -2.08e-02\n",
      "stop_words_per_sentence     -0.06 -2.67e-02 -4.49e-03\n",
      "max_node_depth              -0.06  1.76e-01  6.01e-02\n",
      "avg_words_before_root       -0.05 -3.63e-02 -1.58e-02\n",
      "avg_node_depth              -0.03 -5.57e-02 -1.47e-02\n",
      "stop_words_per_clause       -0.03 -7.92e-02 -2.89e-02\n",
      "avg_num_np_modifiers        -0.03 -6.82e-02 -1.99e-02\n",
      "clauses_per_sent            -0.03 -7.66e-02 -2.25e-02\n",
      "avg_word_freq               -0.03 -2.72e-02  2.35e-03\n",
      "avg_stopless_aoa_max        -0.03 -8.39e-02 -2.90e-02\n",
      "pronouns_per_sent           -0.03 -7.72e-02 -1.83e-02\n",
      "sbars_per_sent              -0.02 -8.05e-02 -2.43e-02\n",
      "avg_stopless_aoa_uniq_max   -0.02 -8.37e-02 -3.27e-02\n",
      "avg_aoa_uniq_max            -0.02 -8.36e-02 -3.66e-02\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "\n",
    "# print(results['num_tokens':])\n",
    "np.set_printoptions(precision=100)\n",
    "\n",
    "results_data = results['num_tokens':].drop(['num_tokens', 'num_words', \n",
    "    'num_sentences', 'num_unk', 'num_uniq_words', 'avg_word_freq_uniq'])\n",
    "# results_data = results['num_tokens':]\n",
    "results_std = StandardScaler().fit_transform(results_data).T\n",
    "# results_std = normalize(results_data).T\n",
    "\n",
    "# print([np.max(i)-np.min(i) for i in results_std])\n",
    "\n",
    "# print(results_std.shape)\n",
    "# fa = FactorAnalysis(n_components=3)\n",
    "# f = fa.fit(results_std).components_.T\n",
    "# # print(np.sum(f.T[0]))\n",
    "# # print(f)\n",
    "\n",
    "# # print(fa.fit_transform(results_std.T))\n",
    "\n",
    "# faf = pd.DataFrame(f, index=list(results_data.index), columns=['1', '2', '3'])\n",
    "# print(faf.nlargest(10, ['1']))\n",
    "\n",
    "\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=3).fit(results_std)\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(sum(pca.explained_variance_ratio_))\n",
    "# print(pca.components_.shape)\n",
    "# print(pca.components_.T)\n",
    "\n",
    "comps = pd.DataFrame(pca.components_.T, index=list(results_data.index), columns=['1', '2', '3'])\n",
    "m = comps.abs().nlargest(20, ['1']).index\n",
    "print(comps.loc[m])\n",
    "\n",
    "print('num_tokens' in comps.index)\n",
    "\n",
    "# print('-'*100)\n",
    "\n",
    "# cov = np.cov(results_std.T)\n",
    "\n",
    "# evals, evecs = np.linalg.eig(cov)\n",
    "\n",
    "# print(evecs)\n",
    "# print(evecs.shape)\n",
    "\n",
    "# explained_variances = []\n",
    "# for i in range(len(evals)):\n",
    "#     explained_variances.append(evals[i] / np.sum(evals))\n",
    " \n",
    "# print(np.sum(explained_variances), '\\n', explained_variances)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- vocab measures\n",
    "    - measures of polysemy\n",
    "- weight ted differently --> should adding/removing be weighted less because it simply indicates a different length sentence?\n",
    "    - how to remove determiners?\n",
    "- make scraper more robust\n",
    "- unit testing for parataxis measures\n",
    "\n",
    "- word not in AoA list \n",
    "    - opt 1: average lemmas\n",
    "    - opt 2: take max\n",
    "\n",
    "- cosine similarity\n",
    "    - Something at the end???\n",
    "\n",
    "- PCA and RFE\n",
    "- maybe paratactic should go back to the regular clause tags because we don't want S:SBAR cuz they're related\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions\n",
    "- Workaround for the spacy max length? <code>ValueError: Sentence of length 965 (in sub-word tokens) exceeds the maximum supported length of 512</code>\n",
    "- How to compare syntax trees better?\n",
    "    - Should I do TED weighting?\n",
    "- Visually displaying / analyzing the data\n",
    "- Code not running on harris..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- View: https://scholarworks.gsu.edu/cgi/viewcontent.cgi?article=1035&context=alesl_diss\n",
    "- View: http://cohmetrix.memphis.edu/cohmetrixhome/documentation_indices.html#Complexity\n",
    "- MORE RESEARCH NEEDED: compare tree similarity (SEARCH TREE EDIT DISTANCE) of sentences in doc, pq-gram distance\n",
    "(CITING APTED: https://pypi.org/project/apted/#description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this that he will go: that i am clear\n",
      "(FRAG (NP (NP (DT this)) (SBAR (WHNP (WDT that)) (S (NP (PRP he)) (VP (MD will) (VP (VB go)))))) (: :) (SBAR (IN that) (S (NP (PRP i)) (VP (VBP am) (ADJP (JJ clear))))))\n",
      "<FRAG><NP><NP><DT>this</DT></NP><SBAR><WHNP><WDT>that</WDT></WHNP><S><NP><PRP>he</PRP></NP><VP><MD>will</MD><VP><VB>go</VB></VP></VP></S></SBAR></NP><PUNCT-COLON>:</PUNCT-COLON><SBAR><IN>that</IN><S><NP><PRP>i</PRP></NP><VP><VBP>am</VBP><ADJP><JJ>clear</JJ></ADJP></VP></S></SBAR></FRAG>\n",
      "[<Element NP at 0x2b1daae16c0>, <Element PUNCT-COLON at 0x2b1da7e5000>, <Element SBAR at 0x2b1da7e4780>]\n",
      "<Element FRAG at 0x2b1da7ab300>\n",
      "[<Element NP at 0x2b1c45ab200>, <Element PUNCT-COLON at 0x2b1da7e47c0>, <Element SBAR at 0x2b1da7e5000>]\n",
      "['NP', 'SBAR']\n",
      "1 0\n"
     ]
    }
   ],
   "source": [
    "p_sum = 0\n",
    "p_suml = 0\n",
    "d = nlp(\"This oath I am now about to take, and in your presence: That if it shall be found during my administration of the Government I have in any instance violated willingly or knowingly the injunctions thereof, I may (besides incurring constitutional punishment) be subject to the upbraidings of all who are now witnesses of the present solemn ceremony.\")\n",
    "d = nlp(\"this that he will go: that i am clear\")\n",
    "# d=nlp(\"to go: that i am clear\")\n",
    "# returns p_sum 2, p_suml 4\n",
    "s = list(d.sents)[0]\n",
    "print(s)\n",
    "xml = sexp_to_xml(s._.parse_string)\n",
    "print(s._.parse_string)\n",
    "print(xml)\n",
    "root = etree.fromstring(xml) \n",
    "tree = etree.ElementTree(root)\n",
    "print(root.getchildren())\n",
    "print(root)\n",
    "\n",
    "p_sum += find_parataxis_strict(root)\n",
    "for e in root.iter():\n",
    "    tag = e.tag\n",
    "    if tag in clause_tags:\n",
    "        p_suml += sum(int(bool(c.tag in parataxis_clause_tags)) for c in e.getchildren())\n",
    "\n",
    "\n",
    "print(p_sum, p_suml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install benepar\n",
    "! pip install spacy\n",
    "! pip install apted\n",
    "! pip install bs4\n",
    "! python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'15.5': 100, '16.5': 100})\n",
      "Counter({'15.5': 200, '14.5': 100, '16.5': 100})\n"
     ]
    }
   ],
   "source": [
    "a=[(\"13.5\",100)]\n",
    "b=[(\"14.5\",100), (\"15.5\", 100)]\n",
    "c=[(\"15.5\",100), (\"16.5\", 100)]\n",
    "b={\"14.5\":100, \"15.5\": 100}\n",
    "c={\"15.5\":100, \"16.5\": 100}\n",
    "input=[b, c]\n",
    "\n",
    "from collections import Counter\n",
    "print(Counter(c))\n",
    "print(sum(\n",
    "    (Counter(x) for x in input),\n",
    "    Counter()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9cc068bd1879d5f924fce910ac6f0b7ba67f436e44a09dbef81e8920d2908e28"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
