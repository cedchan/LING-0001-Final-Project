{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cedch\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package benepar_en3 to\n",
      "[nltk_data]     C:\\Users\\cedch\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package benepar_en3 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import benepar\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import decimal\n",
    "import re\n",
    "from lxml import etree\n",
    "\n",
    "import apted\n",
    "from apted import APTED\n",
    "from apted.helpers import Tree\n",
    "from itertools import combinations\n",
    "from math import comb\n",
    "\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "benepar.download('benepar_en3')\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "if spacy.__version__.startswith('2'):\n",
    "    nlp.add_pipe(benepar.BeneparComponent(\"benepar_en3\"))\n",
    "else:\n",
    "    nlp.add_pipe('benepar', config={'model': 'benepar_en3'})\n",
    "\n",
    "lemmatizer = spacy.load('en_core_web_sm', disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEXP TO XML\n",
    "def clean_xml(xml):\n",
    "    xml = re.sub('<(/?)[^a-zA-Z/][^>]*>', '<\\g<1>UNK>', xml) # invalid tokens labeled 'UNK'\n",
    "    return xml.replace(' ', '')\n",
    "\n",
    "def sexp_to_xml(sexp):\n",
    "    def apply_inner_re(s):\n",
    "        return re.sub('\\(([^ ]*) ([^\\)\\(]*)\\)', '<\\g<1>> \\g<2> </\\g<1>>', s)\n",
    "\n",
    "    xml = apply_inner_re(sexp)\n",
    "    while xml.startswith('('):\n",
    "        xml = apply_inner_re(xml)\n",
    "\n",
    "    with open('special_chars.txt') as f:\n",
    "        special_chars = dict([line.split() for line in f])\n",
    "\n",
    "    def key_to_re(s):\n",
    "        s = re.sub('(.*)([\\\\\\.\\+\\*\\?\\^\\$\\(\\)\\[\\]\\{\\}\\|])(.*)', '\\g<1>\\\\\\\\\\g<2>\\g<3>', s)\n",
    "        return '<(/?)' + s + '>'\n",
    "\n",
    "    for k, v in special_chars.items():\n",
    "        xml = re.sub(key_to_re(k), f'<\\g<1>{v}>', xml)\n",
    "\n",
    "    return clean_xml(xml)\n",
    "\n",
    "# TREE EDIT DISTANCE\n",
    "def apted_format(parse_str):\n",
    "    parse_str = re.sub('\\(([^ ]+) [^ \\(\\)]+?\\)', '(\\g<1>)', parse_str)\n",
    "    parse_str = parse_str.replace(' ', '')\n",
    "    parse_str = parse_str.replace('(', '{')\n",
    "    parse_str = parse_str.replace(')', '}')\n",
    "    return parse_str\n",
    "\n",
    "# PARATACTIC CHILDREN STRICT \n",
    "def find_parataxis_strict(e):\n",
    "    global parataxis_clause_tags\n",
    "    print(e.getchildren())\n",
    "    children = [c.tag for c in e.getchildren() if not(c.tag.startswith('PUNCT-'))] # excludes punct\n",
    "    sum = 0\n",
    "    in_group = False\n",
    "    print(children)\n",
    "    \n",
    "    for i in range(len(children) - 1):\n",
    "        if children[i] in parataxis_clause_tags and children[i + 1] in parataxis_clause_tags:\n",
    "            sum += 1\n",
    "            if not(in_group):\n",
    "                sum += 1\n",
    "                in_group = True\n",
    "        else:\n",
    "            in_group = False\n",
    "    return sum if sum != 0 else 1\n",
    "\n",
    "# LEMMATIZATION\n",
    "def lemmatize(word: str):\n",
    "    return lemmatizer(word)[0].lemma_\n",
    "\n",
    "# AOA\n",
    "def aoa_of(word: str):\n",
    "    global aoa_df, aoa_mode\n",
    "    \n",
    "    search = aoa_df[aoa_df['Word'] == word]['Rating.Mean']\n",
    "    if len(search) == 1:\n",
    "        return float(search)\n",
    "    \n",
    "    lemma_search = aoa_df[aoa_df['Word'] == lemmatize(word)]['Rating.Mean']\n",
    "    if len(lemma_search) == 1:\n",
    "        return float(lemma_search)\n",
    "    \n",
    "    lemmas = aoa_df[aoa_df['Lemma'] == lemmatize(word)]['Rating.Mean']\n",
    "    if len(lemmas) == 0:\n",
    "        return -1\n",
    "    elif len(lemmas) == 1:\n",
    "        return float(lemmas)\n",
    "    elif aoa_mode == 'avg':\n",
    "        return sum(lemmas) / len(lemmas)\n",
    "    elif aoa_mode == 'max':\n",
    "        return max(lemmas)\n",
    "    \n",
    "    return -1\n",
    "\n",
    "#Average\n",
    "def list_avg(l):\n",
    "    return sum(l) / len(l) if len(l) > 0 else 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "clause_tags = ['S', 'SBARQ', 'SINV'] # Not included: 'SQ', 'SBAR'\n",
    "parataxis_clause_tags = ['S', 'SBARQ', 'SINV', 'SQ', 'SBAR']\n",
    "clause_re = re.compile('(' + '/|'.join(clause_tags) + ')')\n",
    "pronoun_tags = ['PRP', 'PRPS']\n",
    "\n",
    "aoa_df = pd.read_csv('reference/aoa/aoa_lemmas.csv')\n",
    "with open('reference/word_frequency/subtlexus_lower.json', encoding='utf-8') as f:\n",
    "    wf_dict = json.load(f)\n",
    "\n",
    "def benepar_analysis(sent):\n",
    "    global num_clauses, num_sbar, num_unk, depth_sum, max_depth, max_clause_depth, clause_depth_sum\n",
    "    global pronoun_sum, num_leaf_nps, num_nps, np_leaf_sum, clause_length_sum\n",
    "    global paratactic_sum, root_parataxis_strict, root_parataxis_loose\n",
    "    paratactic_sum_local = 0\n",
    "\n",
    "    xml = sexp_to_xml(sent._.parse_string)\n",
    "    root = etree.fromstring(xml) \n",
    "    tree = etree.ElementTree(root)\n",
    "\n",
    "    root_parataxis_loose += max(1, sum(int(bool(c.tag in parataxis_clause_tags)) for c in root.getchildren()))\n",
    "    root_parataxis_strict += find_parataxis_strict(root)\n",
    "\n",
    "    num_sbar += sum(int(e.tag == 'SBAR') for e in root.iter())\n",
    "    pronoun_sum += sum(int(e.tag in pronoun_tags) for e in root.iter())\n",
    "    num_unk += sum(int(e.tag == 'UNK') for e in root.iter())\n",
    "    for e in root.iter():\n",
    "        tag = e.tag\n",
    "        if tag in clause_tags: \n",
    "            num_clauses += 1\n",
    "            clause_length_sum += sum(int(not(d.tag.startswith('PUNCT-') and bool(d.text))) for d in e.iterdescendants())\n",
    "            \n",
    "            paratactic_sum_local += sum(int(bool(c.tag in parataxis_clause_tags)) for c in e.getchildren())\n",
    "        elif e.tag == 'NP':\n",
    "            num_nps += 1\n",
    "            is_leaf_np = True\n",
    "            for c in e.iterdescendants():\n",
    "                if c.text: \n",
    "                    if not(c.tag.startswith('PUNCT-') or c.tag == 'DT'): # ignore determiners and punctuation\n",
    "                        np_leaf_sum += 1\n",
    "                else:\n",
    "                    is_leaf_np = False\n",
    "            if is_leaf_np:\n",
    "                num_leaf_nps += 1\n",
    "        \n",
    "        if e.text:\n",
    "            path = tree.getpath(e)\n",
    "\n",
    "            depth = len(re.findall('/', path))\n",
    "            depth_sum += depth # Number of times '/' appears, excluding first\n",
    "            max_depth = max(max_depth, depth)\n",
    "\n",
    "            clause_depth = len(clause_re.findall(path))\n",
    "            clause_depth_sum += clause_depth\n",
    "            max_clause_depth = max(max_clause_depth, clause_depth)\n",
    "    paratactic_sum += max(1, paratactic_sum_local)\n",
    "\n",
    "def spacy_analysis(sent):\n",
    "    global dep_dist_sum, num_words, words_before_root_sum, uniq_words, num_words_no_nums, num_stop_words\n",
    "    global max_dep_dist\n",
    "    global aoa_list, aoa_stopless_list, aoa_uniq_list, aoa_stopless_uniq_list\n",
    "    global wf_list, wf_stopless_list, wf_uniq_list, wf_stopless_uniq_list\n",
    "\n",
    "    num_stop_words += sum(int(token.is_stop) for token in sent)\n",
    "\n",
    "    for token in sent:\n",
    "        if not(token.is_punct or token.is_space):\n",
    "            num_words += 1\n",
    "            dep_dist = abs(token.head.i - token.i)\n",
    "            dep_dist_sum += dep_dist\n",
    "            max_dep_dist = max(dep_dist, max_dep_dist)\n",
    "            \n",
    "            if (wf := wf_dict.get(token.lower_)) is not None:\n",
    "                wf_list.append(wf)\n",
    "                if not token.is_stop:\n",
    "                    wf_stopless_list.append(wf)\n",
    "\n",
    "            if (aoa := aoa_of(token.lower_)) != -1:\n",
    "                aoa_list.append(aoa)\n",
    "                if not token.is_stop:\n",
    "                    aoa_stopless_list.append(aoa)\n",
    "\n",
    "            if token.i < sent.root.i: words_before_root_sum += 1\n",
    "\n",
    "            if not(token.like_num):\n",
    "                if not(token.lower_ in uniq_words):\n",
    "                    uniq_words.append(token.lower_)\n",
    "\n",
    "                    if wf is not None:\n",
    "                        wf_uniq_list.append(wf)\n",
    "                        if not token.is_stop:\n",
    "                            wf_stopless_uniq_list.append(wf)\n",
    "                    \n",
    "                    if aoa != -1:\n",
    "                        aoa_uniq_list.append(aoa)\n",
    "                        if not token.is_stop:\n",
    "                            aoa_stopless_uniq_list.append(aoa)\n",
    "\n",
    "                num_words_no_nums += 1\n",
    "    \n",
    "def ted_analysis(sent1, sent2):\n",
    "    global ted_sum\n",
    "\n",
    "    tree1 = Tree.from_text(apted_format(sent1._.parse_string))\n",
    "    tree2 = Tree.from_text(apted_format(sent2._.parse_string))\n",
    "\n",
    "    apted = APTED(tree1, tree2, )\n",
    "    ted = apted.compute_edit_distance()\n",
    "    ted_sum += ted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this that he will go: that i am clear\n",
      "(FRAG (NP (NP (DT this)) (SBAR (WHNP (WDT that)) (S (NP (PRP he)) (VP (MD will) (VP (VB go)))))) (: :) (SBAR (IN that) (S (NP (PRP i)) (VP (VBP am) (ADJP (JJ clear))))))\n",
      "<FRAG><NP><NP><DT>this</DT></NP><SBAR><WHNP><WDT>that</WDT></WHNP><S><NP><PRP>he</PRP></NP><VP><MD>will</MD><VP><VB>go</VB></VP></VP></S></SBAR></NP><PUNCT-COLON>:</PUNCT-COLON><SBAR><IN>that</IN><S><NP><PRP>i</PRP></NP><VP><VBP>am</VBP><ADJP><JJ>clear</JJ></ADJP></VP></S></SBAR></FRAG>\n",
      "[<Element NP at 0x2b1daae16c0>, <Element PUNCT-COLON at 0x2b1da7e5000>, <Element SBAR at 0x2b1da7e4780>]\n",
      "<Element FRAG at 0x2b1da7ab300>\n",
      "[<Element NP at 0x2b1c45ab200>, <Element PUNCT-COLON at 0x2b1da7e47c0>, <Element SBAR at 0x2b1da7e5000>]\n",
      "['NP', 'SBAR']\n",
      "1 0\n"
     ]
    }
   ],
   "source": [
    "p_sum = 0\n",
    "p_suml = 0\n",
    "d = nlp(\"This oath I am now about to take, and in your presence: That if it shall be found during my administration of the Government I have in any instance violated willingly or knowingly the injunctions thereof, I may (besides incurring constitutional punishment) be subject to the upbraidings of all who are now witnesses of the present solemn ceremony.\")\n",
    "d = nlp(\"this that he will go: that i am clear\")\n",
    "# d=nlp(\"to go: that i am clear\")\n",
    "# returns p_sum 2, p_suml 4\n",
    "s = list(d.sents)[0]\n",
    "print(s)\n",
    "xml = sexp_to_xml(s._.parse_string)\n",
    "print(s._.parse_string)\n",
    "print(xml)\n",
    "root = etree.fromstring(xml) \n",
    "tree = etree.ElementTree(root)\n",
    "print(root.getchildren())\n",
    "print(root)\n",
    "\n",
    "p_sum += find_parataxis_strict(root)\n",
    "for e in root.iter():\n",
    "    tag = e.tag\n",
    "    if tag in clause_tags:\n",
    "        p_suml += sum(int(bool(c.tag in parataxis_clause_tags)) for c in e.getchildren())\n",
    "\n",
    "\n",
    "print(p_sum, p_suml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(NP (DT this) (SBAR (IN that) (S (NP (PRP he)) (VP (MD will) (VP (VB go))))) (: :) (SBAR (IN that) (S (NP (PRP i)) (VP (VBP am) (ADJP (JJ clear))))))\n",
    "(NP (NP (DT this)) (SBAR (WHNP (WDT that)) (S (NP (PRP he)) (VP (MD will) (VP (VB go)))))) (: :) (SBAR (IN that) (S (NP (PRP i)) (VP (VBP am) (ADJP (JJ clear)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fellow-Citizens of the Senate and of the House of Representatives:\n",
      "\n",
      "Among the vicissitudes incident to life no event could have filled me with greater anxieties than that of which the notification was transmitted by your order, and received on the 14th day of the present month. On the one hand, I was summoned by my country, whose voice I can never hear but with veneration and love, from a retreat which I had chosen with the fondest predilection, and, in my flattering hopes, with an immutable decision, as the asylum of my declining years—a retreat which was rendered every day more necessary as well as more dear to me by the addition of habit to inclination, and of frequent interruptions in my health to the gradual waste committed on it by time. On the other hand, the magnitude and difficulty of the trust to which the voice of my country called me, being sufficient to awaken in the wisest and most experienced of her citizens a distrustful scrutiny into his qualifications, could not but overwhelm with despondence one who (inheriting inferior endowments from nature and unpracticed in the duties of civil administration) ought to be peculiarly conscious of his own deficiencies. In this conflict of emotions all I dare aver is that it has been my faithful study to collect my duty from a just appreciation of every circumstance by which it might be affected. All I dare hope is that if, in executing this task, I have been too much swayed by a grateful remembrance of former instances, or by an affectionate sensibility to this transcendent proof of the confidence of my fellow-citizens, and have thence too little consulted my incapacity as well as disinclination for the weighty and untried cares before me, my error will be palliated by the motives which mislead [see APP note] me, and its consequences be judged by my country with some share of the partiality in which they originated.\n",
      "\n",
      "Such being the impressions under which I have, in obedience to the public summons, repaired to the present station, it would be peculiarly improper to omit in this first official act my fervent supplications to that Almighty Being who rules over the universe, who presides in the councils of nations, and whose providential aids can supply every human defect, that His benediction may consecrate to the liberties and happiness of the people of the United States a Government instituted by themselves for these essential purposes, and may enable every instrument employed in its administration to execute with success the functions allotted to his charge. In tendering this homage to the Great Author of every public and private good, I assure myself that it expresses your sentiments not less than my own, nor those of my fellow-citizens at large less than either. No people can be bound to acknowledge and adore the Invisible Hand which conducts the affairs of men more than those of the United States. Every step by which they have advanced to the character of an independent nation seems to have been distinguished by some token of providential agency; and in the important revolution just accomplished in the system of their united government the tranquil deliberations and voluntary consent of so many distinct communities from which the event has resulted can not be compared with the means by which most governments have been established without some return of pious gratitude, along with an humble anticipation of the future blessings which the past seem to presage. These reflections, arising out of the present crisis, have forced themselves too strongly on my mind to be suppressed. You will join with me, I trust, in thinking that there are none under the influence of which the proceedings of a new and free government can more auspiciously commence.\n",
      "\n",
      "By the article establishing the executive department it is made the duty of the President \"to recommend to your consideration such measures as he shall judge necessary and expedient.\" The circumstances under which I now meet you will acquit me from entering into that subject further than to refer to the great constitutional charter under which you are assembled, and which, in defining your powers, designates the objects to which your attention is to be given. It will be more consistent with those circumstances, and far more congenial with the feelings which actuate me, to substitute, in place of a recommendation of particular measures, the tribute that is due to the talents, the rectitude, and the patriotism which adorn the characters selected to devise and adopt them. In these honorable qualifications I behold the surest pledges that as on one side no local prejudices or attachments, no separate views nor party animosities, will misdirect the comprehensive and equal eye which ought to watch over this great assemblage of communities and interests, so, on another, that the foundation of our national policy will be laid in the pure and immutable principles of private morality, and the preeminence of free government be exemplified by all the attributes which can win the affections of its citizens and command the respect of the world. I dwell on this prospect with every satisfaction which an ardent love for my country can inspire, since there is no truth more thoroughly established than that there exists in the economy and course of nature an indissoluble union between virtue and happiness; between duty and advantage; between the genuine maxims of an honest and magnanimous policy and the solid rewards of public prosperity and felicity; since we ought to be no less persuaded that the propitious smiles of Heaven can never be expected on a nation that disregards the eternal rules of order and right which Heaven itself has ordained; and since the preservation of the sacred fire of liberty and the destiny of the republican model of government are justly considered, perhaps, as deeply, as finally, staked on the experiment entrusted to the hands of the American people.\n",
      "\n",
      "Besides the ordinary objects submitted to your care, it will remain with your judgment to decide how far an exercise of the occasional power delegated by the fifth article of the Constitution is rendered expedient at the present juncture by the nature of objections which have been urged against the system, or by the degree of inquietude which has given birth to them. Instead of undertaking particular recommendations on this subject, in which I could be guided by no lights derived from official opportunities, I shall again give way to my entire confidence in your discernment and pursuit of the public good; for I assure myself that whilst you carefully avoid every alteration which might endanger the benefits of an united and effective government, or which ought to await the future lessons of experience, a reverence for the characteristic rights of freemen and a regard for the public harmony will sufficiently influence your deliberations on the question how far the former can be impregnably fortified or the latter be safely and advantageously promoted.\n",
      "\n",
      "To the foregoing observations I have one to add, which will be most properly addressed to the House of Representatives. It concerns myself, and will therefore be as brief as possible. When I was first honored with a call into the service of my country, then on the eve of an arduous struggle for its liberties, the light in which I contemplated my duty required that I should renounce every pecuniary compensation. From this resolution I have in no instance departed; and being still under the impressions which produced it, I must decline as inapplicable to myself any share in the personal emoluments which may be indispensably included in a permanent provision for the executive department, and must accordingly pray that the pecuniary estimates for the station in which I am placed may during my continuance in it be limited to such actual expenditures as the public good may be thought to require.\n",
      "\n",
      "Having thus imparted to you my sentiments as they have been awakened by the occasion which brings us together, I shall take my present leave; but not without resorting once more to the benign Parent of the Human Race in humble supplication that, since He has been pleased to favor the American people with opportunities for deliberating in perfect tranquillity, and dispositions for deciding with unparalleled unanimity on a form of government for the security of their union and the advancement of their happiness, so His divine blessing may be equally conspicuous in the enlarged views, the temperate consultations, and the wise measures on which the success of this Government must depend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cedch\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\distributions\\distribution.py:45: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
      "  warnings.warn(f'{self.__class__} does not define `arg_constraints`. ' +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 27 23\n",
      "Fellow Citizens:\n",
      "\n",
      "I AM again called upon by the voice of my country to execute the functions of its Chief Magistrate. When the occasion proper for it shall arrive, I shall endeavor to express the high sense I entertain of this distinguished honor, and of the confidence which has been reposed in me by the people of united America.\n",
      "\n",
      "Previous to the execution of any official act of the President the Constitution requires an oath of office. This oath I am now about to take, and in your presence: That if it shall be found during my administration of the Government I have in any instance violated willingly or knowingly the injunctions thereof, I may (besides incurring constitutional punishment) be subject to the upbraidings of all who are now witnesses of the present solemn ceremony.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cedch\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\distributions\\distribution.py:45: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
      "  warnings.warn(f'{self.__class__} does not define `arg_constraints`. ' +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 4 4\n",
      "Error Log:\n",
      "\n",
      "                                              1789_Washington_Inaugural_Address  \\\n",
      "date                                                                 1789-04-30   \n",
      "pres_name                                                     George Washington   \n",
      "byline                          1st President of the United States: 1789 ‐ 1797   \n",
      "title                                                         Inaugural Address   \n",
      "benepar_analysis_time                                                      0.01   \n",
      "spacy_analysis_time                                                        0.31   \n",
      "tree_edit_distance_time                                                    9.08   \n",
      "total_file_analysis_time                                                  16.75   \n",
      "num_tokens                                                                 1546   \n",
      "num_sentences                                                                23   \n",
      "avg_tree_edit_dist_adjacent                                              138.23   \n",
      "avg_node_depth                                                            12.68   \n",
      "max_node_depth                                                               50   \n",
      "avg_node_clause_depth                                                      2.46   \n",
      "max_node_clause_depth                                                         8   \n",
      "avg_clause_length                                                         55.32   \n",
      "clauses_per_sent                                                           6.26   \n",
      "sbars_per_sent                                                              3.3   \n",
      "pronouns_per_sent                                                           4.7   \n",
      "pronouns_per_clause                                                        0.75   \n",
      "pronoun_prop_of_leaf_nps                                                   0.31   \n",
      "avg_num_np_modifiers                                                        5.1   \n",
      "loose_parataxis_per_sent                                                   1.39   \n",
      "root_parataxis_per_sent_strict                                              1.0   \n",
      "root_parataxis_per_sent_loose                                              1.17   \n",
      "num_unk                                                                       2   \n",
      "num_words                                                                  1434   \n",
      "avg_dependency_distance                                                    3.56   \n",
      "max_dependency_distance                                                      82   \n",
      "avg_sentence_length_by_tok                                                67.22   \n",
      "avg_sentence_length_by_word                                               62.35   \n",
      "avg_words_before_root                                                     13.48   \n",
      "num_uniq_words                                                              593   \n",
      "proportion_uniq                                                            0.42   \n",
      "stop_words_per_clause                                                      5.92   \n",
      "stop_words_per_sentence                                                   37.09   \n",
      "avg_aoa_max                                                                 6.2   \n",
      "avg_aoa_uniq_max                                                           8.12   \n",
      "avg_stopless_aoa_max                                                       8.62   \n",
      "avg_stopless_aoa_uniq_max                                                  8.84   \n",
      "avg_word_freq                                                           6791.01   \n",
      "avg_word_freq_uniq                                                        835.1   \n",
      "avg_word_freq_stopless                                                    94.66   \n",
      "avg_word_freq_stopless_uniq                                               76.64   \n",
      "\n",
      "                                              1793_Washington_Inaugural_Address  \n",
      "date                                                                 1793-03-04  \n",
      "pres_name                                                     George Washington  \n",
      "byline                          1st President of the United States: 1789 ‐ 1797  \n",
      "title                                                         Inaugural Address  \n",
      "benepar_analysis_time                                                       0.0  \n",
      "spacy_analysis_time                                                        0.16  \n",
      "tree_edit_distance_time                                                    0.17  \n",
      "total_file_analysis_time                                                   0.96  \n",
      "num_tokens                                                                  147  \n",
      "num_sentences                                                                 4  \n",
      "avg_tree_edit_dist_adjacent                                                79.0  \n",
      "avg_node_depth                                                             9.06  \n",
      "max_node_depth                                                               23  \n",
      "avg_node_clause_depth                                                      1.66  \n",
      "max_node_clause_depth                                                         4  \n",
      "avg_clause_length                                                          30.8  \n",
      "clauses_per_sent                                                           3.75  \n",
      "sbars_per_sent                                                             2.25  \n",
      "pronouns_per_sent                                                          3.25  \n",
      "pronouns_per_clause                                                        0.87  \n",
      "pronoun_prop_of_leaf_nps                                                   0.35  \n",
      "avg_num_np_modifiers                                                       4.11  \n",
      "loose_parataxis_per_sent                                                    1.0  \n",
      "root_parataxis_per_sent_strict                                              1.0  \n",
      "root_parataxis_per_sent_loose                                               1.0  \n",
      "num_unk                                                                       0  \n",
      "num_words                                                                   135  \n",
      "avg_dependency_distance                                                    3.07  \n",
      "max_dependency_distance                                                      46  \n",
      "avg_sentence_length_by_tok                                                36.75  \n",
      "avg_sentence_length_by_word                                               33.75  \n",
      "avg_words_before_root                                                      16.5  \n",
      "num_uniq_words                                                               90  \n",
      "proportion_uniq                                                            0.67  \n",
      "stop_words_per_clause                                                      5.27  \n",
      "stop_words_per_sentence                                                   19.75  \n",
      "avg_aoa_max                                                                6.05  \n",
      "avg_aoa_uniq_max                                                           6.92  \n",
      "avg_stopless_aoa_max                                                       8.58  \n",
      "avg_stopless_aoa_uniq_max                                                  8.52  \n",
      "avg_word_freq                                                           8143.49  \n",
      "avg_word_freq_uniq                                                      2832.02  \n",
      "avg_word_freq_stopless                                                     78.1  \n",
      "avg_word_freq_stopless_uniq                                                75.3  \n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame([])\n",
    "\n",
    "# TED Mode\n",
    "#  - 'combinations': Averages the TED of every pair of sentences in a doc\n",
    "#  - 'adjacent': Averages the TED of a sentence and the next sentence in a doc\n",
    "ted_mode = 'adjacent'\n",
    "# AoA Mode: If word is not in AoA list and multiple lemma matches exist\n",
    "#  - 'max': Chooses max of lemma matches\n",
    "#  - 'avg': Averages lemma matches\n",
    "aoa_mode = 'max'\n",
    "log = \"\"\n",
    "\n",
    "for i, file in enumerate(os.scandir('data/text_jsons/')):\n",
    "    if i == 2: break\n",
    "    \n",
    "    file_time = time.perf_counter()\n",
    "    file_name = re.sub('\\.json$', '', file.name)\n",
    "    with open(file, encoding='utf-8') as f:\n",
    "        metadata = json.load(f)\n",
    "    text = metadata['text']\n",
    "    print(text)\n",
    "    text = text.replace('\\n', ' ').strip()\n",
    "    text = re.sub('\\s{2,}', ' ', text)\n",
    "    \n",
    "    try:\n",
    "        doc = nlp(text)\n",
    "    except ValueError:\n",
    "        error_text = f\"ValueError in '{file_name}'. Likely exists too long sentence. Skipping.\"\n",
    "        log += error_text + '\\n'\n",
    "        continue\n",
    "    except:\n",
    "        error_text = f\"Some other error occured in '{file_name}'. Skipping.\"\n",
    "        log += error_text + '\\n'\n",
    "        continue\n",
    "\n",
    "    sents = list(doc.sents)\n",
    "\n",
    "    # Doc-level\n",
    "    num_tokens = len(doc)\n",
    "    num_sents = len(sents)\n",
    "    ted_sum = 0\n",
    "\n",
    "    # Benepar\n",
    "    num_clauses = 0\n",
    "    num_sbar = 0\n",
    "    num_unk = 0\n",
    "    depth_sum = 0\n",
    "    max_depth = 0\n",
    "    clause_depth_sum = 0\n",
    "    max_clause_depth = 0\n",
    "    clause_length_sum = 0\n",
    "    pronoun_sum = 0\n",
    "    num_leaf_nps = 0 # all children are leaves\n",
    "    num_nps = 0 # all NPs\n",
    "    np_leaf_sum = 0 # number of leaf descendents a NP has (i.e., number of modifying words)\n",
    "    # includes coord conj, goes through ever node, only counts children, min 1 per sent\n",
    "    paratactic_sum = 0 \n",
    "    root_parataxis_strict = 0 # only looks at punctuation-separated children of root, 1 if none\n",
    "    root_parataxis_loose = 0 # includes coordinating conj, only root, 1 if none\n",
    "\n",
    "    # spaCy\n",
    "    dep_dist_sum = 0\n",
    "    max_dep_dist = 0\n",
    "    num_words = 0\n",
    "    num_words_no_nums = 0\n",
    "    uniq_words = []\n",
    "    words_before_root_sum = 0 # Root as in word whose head is self\n",
    "    num_stop_words = 0\n",
    "    aoa_list = []\n",
    "    aoa_uniq_list = []\n",
    "    aoa_stopless_list = []\n",
    "    aoa_stopless_uniq_list = []\n",
    "    wf_list = []\n",
    "    wf_uniq_list = []\n",
    "    wf_stopless_list = []\n",
    "    wf_stopless_uniq_list = []\n",
    "\n",
    "    for sent in sents:\n",
    "        benepar_time = time.perf_counter()\n",
    "        benepar_analysis(sent)\n",
    "        benepar_time = time.perf_counter() - benepar_time\n",
    "\n",
    "        spacy_time = time.perf_counter()\n",
    "        spacy_analysis(sent)\n",
    "        spacy_time = time.perf_counter() - spacy_time\n",
    "\n",
    "    # TREE EDIT DISTANCE\n",
    "    ted_time = time.perf_counter()\n",
    "    if ted_mode == 'adjacent':    \n",
    "        for i in range(num_sents - 1):\n",
    "            ted_analysis(sents[i], sents[i + 1])\n",
    "        ted_avg = ted_sum / (num_sents - 1)\n",
    "    elif ted_mode == 'combinations':\n",
    "        for sent1, sent2 in combinations(sents, 2):\n",
    "            ted_analysis(sent1, sent2)\n",
    "        ted_avg = ted_sum / comb(num_sents, 2)\n",
    "    else:\n",
    "        print('Invalid ted_mode:', ted_mode)\n",
    "        ted_avg = -1\n",
    "    ted_time = time.perf_counter() - ted_time\n",
    "\n",
    "    summary = {\n",
    "        # File-level\n",
    "        'date' : metadata['date'],\n",
    "        'pres_name' : metadata['pres_name'],\n",
    "        'byline' : metadata['byline'],\n",
    "        'title' : metadata['title'],\n",
    "\n",
    "        # Performance time\n",
    "        'benepar_analysis_time' : benepar_time,\n",
    "        'spacy_analysis_time' : spacy_time,\n",
    "        'tree_edit_distance_time' : ted_time,\n",
    "        'total_file_analysis_time' : time.perf_counter() - file_time,\n",
    "\n",
    "        # Doc-level\n",
    "        'num_tokens' : num_tokens,\n",
    "        'num_sentences' : num_sents, \n",
    "        # 'avg_ted_adj' : ted_avg_adj,\n",
    "        # 'avg_ted_comb' : ted_avg_comb,\n",
    "        f'avg_tree_edit_dist_{ted_mode}' : ted_avg,\n",
    "\n",
    "        # Benepar\n",
    "        'avg_node_depth' : depth_sum / num_tokens, \n",
    "        'max_node_depth' : max_depth, # Equivalent to tree height\n",
    "        'avg_node_clause_depth' : clause_depth_sum / num_tokens,\n",
    "        'max_node_clause_depth' : max_clause_depth,\n",
    "        'avg_clause_length' : clause_length_sum / num_clauses,\n",
    "        'clauses_per_sent' : num_clauses / num_sents, \n",
    "        'sbars_per_sent' : num_sbar / num_sents,\n",
    "        'pronouns_per_sent' : pronoun_sum / num_sents,\n",
    "        'pronouns_per_clause' : pronoun_sum / num_clauses,\n",
    "        'pronoun_prop_of_leaf_nps' : pronoun_sum / num_leaf_nps,\n",
    "        'avg_num_np_modifiers' : np_leaf_sum / num_nps,\n",
    "        'loose_parataxis_per_sent' : paratactic_sum / num_sents,\n",
    "        'root_parataxis_per_sent_strict' : root_parataxis_strict / num_sents,\n",
    "        'root_parataxis_per_sent_loose' : root_parataxis_loose / num_sents,\n",
    "        'num_unk' : num_unk,\n",
    "\n",
    "        # spaCy\n",
    "        'num_words' : num_words,\n",
    "        'avg_dependency_distance' : dep_dist_sum / num_words,\n",
    "        'max_dependency_distance' : max_dep_dist,\n",
    "        'avg_sentence_length_by_tok' : num_tokens / num_sents, \n",
    "        'avg_sentence_length_by_word' : num_words / num_sents,\n",
    "        'avg_words_before_root' : words_before_root_sum / num_sents,\n",
    "        'num_uniq_words' : len(uniq_words), \n",
    "        'proportion_uniq' : len(uniq_words) / num_words_no_nums,\n",
    "        'stop_words_per_clause' : num_stop_words / num_clauses,\n",
    "        'stop_words_per_sentence' : num_stop_words / num_sents,\n",
    "        f'avg_aoa_{aoa_mode}' : list_avg(aoa_list),\n",
    "        f'avg_aoa_uniq_{aoa_mode}' : list_avg(aoa_uniq_list),\n",
    "        f'avg_stopless_aoa_{aoa_mode}' : list_avg(aoa_stopless_list),\n",
    "        f'avg_stopless_aoa_uniq_{aoa_mode}' : list_avg(aoa_stopless_uniq_list),\n",
    "        'avg_word_freq' : list_avg(wf_list),\n",
    "        'avg_word_freq_uniq' : list_avg(wf_uniq_list),\n",
    "        'avg_word_freq_stopless' : list_avg(wf_stopless_list),\n",
    "        'avg_word_freq_stopless_uniq' : list_avg(wf_stopless_uniq_list),\n",
    "    }      \n",
    "    \n",
    "    results[file_name] = summary\n",
    "    \n",
    "    print(paratactic_sum, root_parataxis_loose, root_parataxis_strict)\n",
    "\n",
    "print(\"Error Log:\")\n",
    "print(log)\n",
    "pd.set_option('display.precision', 2)\n",
    "print(results)\n",
    "results.to_csv(f\"results/{datetime.now().strftime('%m-%d-%Y_%H-%M')}.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.798829799226471   0.13868336305891654 0.03236415372895239]\n",
      "0.96987731601434\n",
      "                                1         2         3\n",
      "avg_word_freq_stopless       0.84  1.72e-01 -4.53e-01\n",
      "avg_word_freq_stopless_uniq  0.43  4.56e-02  8.78e-01\n",
      "avg_tree_edit_dist_adjacent -0.18  2.06e-01  6.46e-02\n",
      "max_dependency_distance     -0.17  8.83e-01 -2.74e-02\n",
      "avg_sentence_length_by_tok  -0.10  1.34e-02 -1.67e-03\n",
      "avg_sentence_length_by_word -0.10  3.90e-03 -2.90e-03\n",
      "avg_clause_length           -0.08  7.60e-03 -2.08e-02\n",
      "stop_words_per_sentence     -0.06 -2.67e-02 -4.49e-03\n",
      "max_node_depth              -0.06  1.76e-01  6.01e-02\n",
      "avg_words_before_root       -0.05 -3.63e-02 -1.58e-02\n",
      "avg_node_depth              -0.03 -5.57e-02 -1.47e-02\n",
      "stop_words_per_clause       -0.03 -7.92e-02 -2.89e-02\n",
      "avg_num_np_modifiers        -0.03 -6.82e-02 -1.99e-02\n",
      "clauses_per_sent            -0.03 -7.66e-02 -2.25e-02\n",
      "avg_word_freq               -0.03 -2.72e-02  2.35e-03\n",
      "avg_stopless_aoa_max        -0.03 -8.39e-02 -2.90e-02\n",
      "pronouns_per_sent           -0.03 -7.72e-02 -1.83e-02\n",
      "sbars_per_sent              -0.02 -8.05e-02 -2.43e-02\n",
      "avg_stopless_aoa_uniq_max   -0.02 -8.37e-02 -3.27e-02\n",
      "avg_aoa_uniq_max            -0.02 -8.36e-02 -3.66e-02\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "\n",
    "# print(results['num_tokens':])\n",
    "np.set_printoptions(precision=100)\n",
    "\n",
    "results_data = results['num_tokens':].drop(['num_tokens', 'num_words', \n",
    "    'num_sentences', 'num_unk', 'num_uniq_words', 'avg_word_freq_uniq'])\n",
    "# results_data = results['num_tokens':]\n",
    "results_std = StandardScaler().fit_transform(results_data).T\n",
    "# results_std = normalize(results_data).T\n",
    "\n",
    "# print([np.max(i)-np.min(i) for i in results_std])\n",
    "\n",
    "# print(results_std.shape)\n",
    "# fa = FactorAnalysis(n_components=3)\n",
    "# f = fa.fit(results_std).components_.T\n",
    "# # print(np.sum(f.T[0]))\n",
    "# # print(f)\n",
    "\n",
    "# # print(fa.fit_transform(results_std.T))\n",
    "\n",
    "# faf = pd.DataFrame(f, index=list(results_data.index), columns=['1', '2', '3'])\n",
    "# print(faf.nlargest(10, ['1']))\n",
    "\n",
    "\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=3).fit(results_std)\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(sum(pca.explained_variance_ratio_))\n",
    "# print(pca.components_.shape)\n",
    "# print(pca.components_.T)\n",
    "\n",
    "comps = pd.DataFrame(pca.components_.T, index=list(results_data.index), columns=['1', '2', '3'])\n",
    "m = comps.abs().nlargest(20, ['1']).index\n",
    "print(comps.loc[m])\n",
    "\n",
    "print('num_tokens' in comps.index)\n",
    "\n",
    "# print('-'*100)\n",
    "\n",
    "# cov = np.cov(results_std.T)\n",
    "\n",
    "# evals, evecs = np.linalg.eig(cov)\n",
    "\n",
    "# print(evecs)\n",
    "# print(evecs.shape)\n",
    "\n",
    "# explained_variances = []\n",
    "# for i in range(len(evals)):\n",
    "#     explained_variances.append(evals[i] / np.sum(evals))\n",
    " \n",
    "# print(np.sum(explained_variances), '\\n', explained_variances)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- vocab measures\n",
    "    - measures of polysemy\n",
    "- weight ted differently --> should adding/removing be weighted less because it simply indicates a different length sentence?\n",
    "    - how to remove determiners?\n",
    "- make scraper more robust\n",
    "- unit testing for parataxis measures\n",
    "\n",
    "- word not in AoA list \n",
    "    - opt 1: average lemmas\n",
    "    - opt 2: take max\n",
    "\n",
    "- cosine similarity\n",
    "    - Something at the end???\n",
    "\n",
    "- PCA and RFE\n",
    "- maybe paratactic should go back to the regular clause tags because we don't want S:SBAR cuz they're related\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions\n",
    "- Workaround for the spacy max length? <code>ValueError: Sentence of length 965 (in sub-word tokens) exceeds the maximum supported length of 512</code>\n",
    "- How to compare syntax trees better?\n",
    "    - Should I do TED weighting?\n",
    "- Visually displaying / analyzing the data\n",
    "- Code not running on harris..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- View: https://scholarworks.gsu.edu/cgi/viewcontent.cgi?article=1035&context=alesl_diss\n",
    "- View: http://cohmetrix.memphis.edu/cohmetrixhome/documentation_indices.html#Complexity\n",
    "- MORE RESEARCH NEEDED: compare tree similarity (SEARCH TREE EDIT DISTANCE) of sentences in doc, pq-gram distance\n",
    "(CITING APTED: https://pypi.org/project/apted/#description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install benepar\n",
    "! pip install spacy\n",
    "! pip install apted\n",
    "! pip install bs4\n",
    "! python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'15.5': 100, '16.5': 100})\n",
      "Counter({'15.5': 200, '14.5': 100, '16.5': 100})\n"
     ]
    }
   ],
   "source": [
    "a=[(\"13.5\",100)]\n",
    "b=[(\"14.5\",100), (\"15.5\", 100)]\n",
    "c=[(\"15.5\",100), (\"16.5\", 100)]\n",
    "b={\"14.5\":100, \"15.5\": 100}\n",
    "c={\"15.5\":100, \"16.5\": 100}\n",
    "input=[b, c]\n",
    "\n",
    "from collections import Counter\n",
    "print(Counter(c))\n",
    "print(sum(\n",
    "    (Counter(x) for x in input),\n",
    "    Counter()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9cc068bd1879d5f924fce910ac6f0b7ba67f436e44a09dbef81e8920d2908e28"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
