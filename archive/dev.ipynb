{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cedch\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package benepar_en3 to\n",
      "[nltk_data]     C:\\Users\\cedch\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package benepar_en3 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import benepar\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import decimal\n",
    "from collections import Counter\n",
    "import re\n",
    "from lxml import etree\n",
    "\n",
    "import apted\n",
    "from apted import APTED\n",
    "from apted.helpers import Tree\n",
    "from itertools import combinations\n",
    "\n",
    "from math import comb\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "benepar.download('benepar_en3')\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "if spacy.__version__.startswith('2'):\n",
    "    nlp.add_pipe(benepar.BeneparComponent(\"benepar_en3\"))\n",
    "else:\n",
    "    nlp.add_pipe('benepar', config={'model': 'benepar_en3'})\n",
    "\n",
    "lemmatizer = spacy.load('en_core_web_sm', disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEXP TO XML\n",
    "def clean_xml(xml):\n",
    "    xml = re.sub('<(/?)[^a-zA-Z/][^>]*>', '<\\g<1>UNK>', xml) # invalid tokens labeled 'UNK'\n",
    "    return xml.replace(' ', '')\n",
    "\n",
    "def sexp_to_xml(sexp):\n",
    "    def apply_inner_re(s):\n",
    "        return re.sub('\\(([^ ]*) ([^\\)\\(]*)\\)', '<\\g<1>> \\g<2> </\\g<1>>', s)\n",
    "\n",
    "    xml = apply_inner_re(sexp)\n",
    "    while xml.startswith('('):\n",
    "        xml = apply_inner_re(xml)\n",
    "\n",
    "    with open('special_chars.txt') as f:\n",
    "        special_chars = dict([line.split() for line in f])\n",
    "\n",
    "    def key_to_re(s):\n",
    "        s = re.sub('(.*)([\\\\\\.\\+\\*\\?\\^\\$\\(\\)\\[\\]\\{\\}\\|])(.*)', '\\g<1>\\\\\\\\\\g<2>\\g<3>', s)\n",
    "        return '<(/?)' + s + '>'\n",
    "\n",
    "    for k, v in special_chars.items():\n",
    "        xml = re.sub(key_to_re(k), f'<\\g<1>{v}>', xml)\n",
    "\n",
    "    return clean_xml(xml)\n",
    "\n",
    "# TREE EDIT DISTANCE\n",
    "def apted_format(parse_str):\n",
    "    parse_str = re.sub('\\(([^ ]+) [^ \\(\\)]+?\\)', '(\\g<1>)', parse_str)\n",
    "    parse_str = parse_str.replace(' ', '')\n",
    "    parse_str = parse_str.replace('(', '{')\n",
    "    parse_str = parse_str.replace(')', '}')\n",
    "    return parse_str\n",
    "\n",
    "# PARATACTIC CHILDREN STRICT \n",
    "def find_parataxis_strict(e):\n",
    "    global parataxis_clause_tags\n",
    "    print(e.getchildren())\n",
    "    children = [c.tag for c in e.getchildren() if not(c.tag.startswith('PUNCT-'))] # excludes punct\n",
    "    sum = 0\n",
    "    in_group = False\n",
    "    print(children)\n",
    "    \n",
    "    for i in range(len(children) - 1):\n",
    "        if children[i] in parataxis_clause_tags and children[i + 1] in parataxis_clause_tags:\n",
    "            sum += 1\n",
    "            if not(in_group):\n",
    "                sum += 1\n",
    "                in_group = True\n",
    "        else:\n",
    "            in_group = False\n",
    "    return sum if sum != 0 else 1\n",
    "\n",
    "# LEMMATIZATION\n",
    "def lemmatize(word: str):\n",
    "    return lemmatizer(word)[0].lemma_\n",
    "\n",
    "# AOA\n",
    "def aoa_of(word: str):\n",
    "    global aoa_df, aoa_mode\n",
    "    \n",
    "    search = aoa_df[aoa_df['Word'] == word]['Rating.Mean']\n",
    "    if len(search) == 1:\n",
    "        return float(search)\n",
    "    \n",
    "    lemma_search = aoa_df[aoa_df['Word'] == lemmatize(word)]['Rating.Mean']\n",
    "    if len(lemma_search) == 1:\n",
    "        return float(lemma_search)\n",
    "    \n",
    "    lemmas = aoa_df[aoa_df['Lemma'] == lemmatize(word)]['Rating.Mean']\n",
    "    if len(lemmas) == 0:\n",
    "        return -1\n",
    "    elif len(lemmas) == 1:\n",
    "        return float(lemmas)\n",
    "    elif aoa_mode == 'avg':\n",
    "        return sum(lemmas) / len(lemmas)\n",
    "    elif aoa_mode == 'max':\n",
    "        return max(lemmas)\n",
    "    \n",
    "    return -1\n",
    "\n",
    "#Average\n",
    "def list_avg(l):\n",
    "    return sum(l) / len(l) if len(l) > 0 else 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "clause_tags = ['S', 'SBARQ', 'SINV'] # Not included: 'SQ', 'SBAR'\n",
    "parataxis_clause_tags = ['S', 'SBARQ', 'SINV', 'SQ', 'SBAR']\n",
    "clause_re = re.compile('(' + '/|'.join(clause_tags) + ')')\n",
    "pronoun_tags = ['PRP', 'PRPS']\n",
    "\n",
    "aoa_df = pd.read_csv('reference/aoa/aoa_lemmas.csv')\n",
    "with open('reference/word_frequency/subtlexus_lower.json', encoding='utf-8') as f:\n",
    "    wf_dict = json.load(f)\n",
    "\n",
    "def benepar_analysis(sent):\n",
    "    features_lst = ['num_clauses', 'num_sbar', 'num_unk', 'depth_sum', 'clause_depth_sum', 'pronoun_sum', 'num_leaf_nps', 'num_nps', \n",
    "        'np_leaf_sum', 'clause_length_sum', 'paratactic_sum', 'root_parataxis_strict', \n",
    "        'root_parataxis_loose']\n",
    "    features = Counter({x : 0 for x in features_lst}) # initialize to 0\n",
    "    max_features = {\n",
    "        'max_clause_depth' : 0,\n",
    "        'max_depth' : 0, \n",
    "    }\n",
    "\n",
    "    paratactic_sum_local = 0\n",
    "\n",
    "    xml = sexp_to_xml(sent._.parse_string)\n",
    "    root = etree.fromstring(xml) \n",
    "    tree = etree.ElementTree(root)\n",
    "\n",
    "    features['root_parataxis_loose'] = max(1, sum(int(bool(c.tag in parataxis_clause_tags)) \n",
    "                                                                    for c in root.getchildren()))\n",
    "    features['root_parataxis_strict'] = find_parataxis_strict(root)\n",
    "    features['num_sbar'] = sum(int(e.tag == 'SBAR') for e in root.iter())\n",
    "    features['pronoun_sum'] = sum(int(e.tag in pronoun_tags) for e in root.iter())\n",
    "    features['num_unk'] = sum(int(e.tag == 'UNK') for e in root.iter())\n",
    "    \n",
    "    for e in root.iter():\n",
    "        tag = e.tag\n",
    "        if tag in clause_tags: \n",
    "            features['num_clauses'] += 1\n",
    "            features['clause_length_sum'] += sum(int(not(d.tag.startswith('PUNCT-') and bool(d.text))) for d in e.iterdescendants())\n",
    "            \n",
    "            paratactic_sum_local += sum(int(bool(c.tag in parataxis_clause_tags)) for c in e.getchildren())\n",
    "        elif e.tag == 'NP':\n",
    "            features['num_nps'] += 1\n",
    "            is_leaf_np = True\n",
    "            for c in e.iterdescendants():\n",
    "                if c.text: \n",
    "                    if not(c.tag.startswith('PUNCT-') or c.tag == 'DT'): # ignore determiners and punctuation\n",
    "                        features['np_leaf_sum'] += 1\n",
    "                else:\n",
    "                    is_leaf_np = False\n",
    "            if is_leaf_np:\n",
    "                features['num_leaf_nps'] += 1\n",
    "        \n",
    "        if e.text:\n",
    "            path = tree.getpath(e)\n",
    "\n",
    "            depth = len(re.findall('/', path))\n",
    "            features['depth_sum'] += depth # Number of times '/' appears, excluding first\n",
    "            max_features['max_depth'] = max(max_features['max_depth'], depth)\n",
    "\n",
    "            clause_depth = len(clause_re.findall(path))\n",
    "            features['clause_depth_sum'] += clause_depth\n",
    "            max_features['max_clause_depth'] = max(max_features['max_clause_depth'], clause_depth)\n",
    "            if clause_depth == 8:\n",
    "                print\n",
    "            \n",
    "    features['paratactic_sum'] = max(1, paratactic_sum_local)\n",
    "\n",
    "    return features, max_features\n",
    "\n",
    "def spacy_analysis(sent, uniq_words: set):\n",
    "    features_lst = ['dep_dist_sum', 'num_words', 'words_before_root_sum', 'uniq_words', \n",
    "        'num_words_no_nums', 'num_stop_words', 'aoa_sum', 'aoa_count', \n",
    "        'aoa_stopless_sum', 'aoa_stopless_count', 'aoa_uniq_sum', 'aoa_uniq_count',\n",
    "        'aoa_stopless_uniq_sum', 'aoa_stopless_uniq_count', 'wf_sum', 'wf_count', \n",
    "        'wf_stopless_sum', 'wf_stopless_count', 'wf_uniq_sum', 'wf_uniq_count',\n",
    "        'wf_stopless_uniq_sum', 'wf_stopless_uniq_count']\n",
    "    features = Counter({x : 0 for x in features_lst})\n",
    "    max_features = {\n",
    "        'max_dep_dist': 0, \n",
    "    }\n",
    "\n",
    "    features['num_stop_words'] = sum(int(token.is_stop) for token in sent)\n",
    "\n",
    "    for token in sent:\n",
    "        if not(token.is_punct or token.is_space):\n",
    "            features['num_words'] += 1\n",
    "            dep_dist = abs(token.head.i - token.i)\n",
    "            features['dep_dist_sum'] += dep_dist\n",
    "            max_features['max_dep_dist'] = max(dep_dist, max_features['max_dep_dist'])\n",
    "            \n",
    "            if (wf := wf_dict.get(token.lower_)) is not None:\n",
    "                features['wf_sum'] += wf\n",
    "                features['wf_count'] += 1\n",
    "                if not token.is_stop:\n",
    "                    features['wf_stopless_sum'] += wf\n",
    "                    features['wf_stopless_count'] += 1\n",
    "\n",
    "            if (aoa := aoa_of(token.lower_)) != -1:\n",
    "                features['aoa_sum'] += aoa\n",
    "                features['aoa_count'] += 1\n",
    "                if not token.is_stop:\n",
    "                    features['aoa_stopless_sum'] += aoa\n",
    "                    features['aoa_stopless_count'] += 1\n",
    "\n",
    "            if token.i < sent.root.i: \n",
    "                features['words_before_root_sum'] += 1\n",
    "\n",
    "            if not(token.like_num):\n",
    "                if not(token.lower_ in uniq_words):\n",
    "                    uniq_words.add(token.lower_)\n",
    "\n",
    "                    if wf is not None:\n",
    "                        features['wf_uniq_sum'] += wf\n",
    "                        features['wf_uniq_count'] += 1\n",
    "                        if not token.is_stop:\n",
    "                            features['wf_stopless_uniq_sum'] += wf\n",
    "                            features['wf_stopless_uniq_count'] += 1\n",
    "                    \n",
    "                    if aoa != -1:\n",
    "                        features['aoa_uniq_sum'] += aoa\n",
    "                        features['aoa_uniq_count'] += 1\n",
    "                        if not token.is_stop:\n",
    "                            features['aoa_stopless_uniq_sum'] += aoa\n",
    "                            features['aoa_stopless_uniq_count'] += 1\n",
    "\n",
    "                features['num_words_no_nums'] += 1\n",
    "    return features, max_features, uniq_words\n",
    "    \n",
    "def ted_analysis(sent1, sent2):\n",
    "    global ted_sum\n",
    "\n",
    "    tree1 = Tree.from_text(apted_format(sent1._.parse_string))\n",
    "    tree2 = Tree.from_text(apted_format(sent2._.parse_string))\n",
    "\n",
    "    apted = APTED(tree1, tree2, )\n",
    "    ted = apted.compute_edit_distance()\n",
    "    ted_sum += ted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(NP (DT this) (SBAR (IN that) (S (NP (PRP he)) (VP (MD will) (VP (VB go))))) (: :) (SBAR (IN that) (S (NP (PRP i)) (VP (VBP am) (ADJP (JJ clear))))))\n",
    "(NP (NP (DT this)) (SBAR (WHNP (WDT that)) (S (NP (PRP he)) (VP (MD will) (VP (VB go)))))) (: :) (SBAR (IN that) (S (NP (PRP i)) (VP (VBP am) (ADJP (JJ clear)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cedch\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\distributions\\distribution.py:45: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
      "  warnings.warn(f'{self.__class__} does not define `arg_constraints`. ' +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Element NP at 0x2b1da999640>, <Element PUNCT-COLON at 0x2b1da9a7000>, <Element S at 0x2b1c4535400>, <Element PUNCT-PERIOD at 0x2b1d5aa7b40>]\n",
      "['NP', 'S']\n",
      "[<Element PP at 0x2b1d48a7080>, <Element PUNCT-COMMA at 0x2b1da1692c0>, <Element NP at 0x2b1d5a71f80>, <Element VP at 0x2b1c4534d80>, <Element PUNCT-PERIOD at 0x2b1de1df5c0>]\n",
      "['PP', 'NP', 'VP']\n",
      "[<Element PP at 0x2b20d58e880>, <Element PUNCT-COMMA at 0x2b20d58f9c0>, <Element NP at 0x2b20d58de00>, <Element VP at 0x2b20d58d000>, <Element PUNCT-PERIOD at 0x2b20d58c040>]\n",
      "['PP', 'NP', 'VP']\n",
      "[<Element PP at 0x2b20d58d740>, <Element NP at 0x2b1da2d8500>, <Element VP at 0x2b1b5dc84c0>, <Element PUNCT-PERIOD at 0x2b1b5dcb440>]\n",
      "['PP', 'NP', 'VP']\n",
      "[<Element NP at 0x2b1dead4980>, <Element VP at 0x2b1dead5740>, <Element PUNCT-PERIOD at 0x2b1dead78c0>]\n",
      "['NP', 'VP']\n",
      "[<Element PP at 0x2b1dc116b80>, <Element PUNCT-COMMA at 0x2b2160f1100>, <Element NP at 0x2b1de0e14c0>, <Element VP at 0x2b1de0e06c0>, <Element PUNCT-PERIOD at 0x2b1de0e1d00>]\n",
      "['PP', 'NP', 'VP']\n",
      "[<Element PP at 0x2b1de0e2580>, <Element PUNCT-COMMA at 0x2b1de0e3700>, <Element NP at 0x2b1de0e04c0>, <Element VP at 0x2b1de0e2380>, <Element PUNCT-PERIOD at 0x2b1de0e3300>]\n",
      "['PP', 'NP', 'VP']\n",
      "[<Element NP at 0x2b1da0bf5c0>, <Element VP at 0x2b1da0be840>, <Element PUNCT-PERIOD at 0x2b1da0bf400>]\n",
      "['NP', 'VP']\n",
      "[<Element S at 0x2b1de931300>, <Element PUNCT-COLON at 0x2b1de933400>, <Element CC at 0x2b1de930540>, <Element S at 0x2b1de930d80>, <Element PUNCT-PERIOD at 0x2b1de933e00>]\n",
      "['S', 'CC', 'S']\n",
      "[<Element NP at 0x2b1da157880>, <Element VP at 0x2b1da156d80>, <Element PUNCT-PERIOD at 0x2b1da156980>]\n",
      "['NP', 'VP']\n",
      "[<Element NP at 0x2b20d5c6d00>, <Element VP at 0x2b20d5c73c0>, <Element PUNCT-PERIOD at 0x2b20d5c75c0>]\n",
      "['NP', 'VP']\n",
      "[<Element PP at 0x2b1da1579c0>, <Element NP at 0x2b1da157980>, <Element VP at 0x2b22f1cfd00>, <Element PUNCT-PERIOD at 0x2b1da1d7b00>, <Element UNK at 0x2b1da1d4800>]\n",
      "['PP', 'NP', 'VP', 'UNK']\n",
      "[<Element NP at 0x2b1da1d7e40>, <Element VP at 0x2b1da1d4800>, <Element PUNCT-PERIOD at 0x2b1da1d7a80>]\n",
      "['NP', 'VP']\n",
      "[<Element NP at 0x2b1ae49b700>, <Element VP at 0x2b1da1d4680>, <Element PUNCT-PERIOD at 0x2b1da1d49c0>]\n",
      "['NP', 'VP']\n",
      "[<Element PP at 0x2b1da1d7dc0>, <Element NP at 0x2b1da1cf5c0>, <Element VP at 0x2b1da1cdc40>, <Element PUNCT-PERIOD at 0x2b1da1ce840>]\n",
      "['PP', 'NP', 'VP']\n",
      "[<Element NP at 0x2b1ae188440>, <Element VP at 0x2b1ae189400>, <Element PUNCT-PERIOD at 0x2b1ae18b1c0>]\n",
      "['NP', 'VP']\n",
      "[<Element PP at 0x2b1da19fb40>, <Element PUNCT-COMMA at 0x2b1da19ffc0>, <Element NP at 0x2b1da19f240>, <Element VP at 0x2b1da19c940>, <Element PUNCT-PERIOD at 0x2b1da19cbc0>]\n",
      "['PP', 'NP', 'VP']\n",
      "[<Element S at 0x2b1da19c1c0>, <Element PUNCT-COLON at 0x2b1da19f540>, <Element IN at 0x2b1da19ce80>, <Element S at 0x2b1da19df40>, <Element PUNCT-PERIOD at 0x2b1da19f980>]\n",
      "['S', 'IN', 'S']\n",
      "[<Element PP at 0x2b1da19e380>, <Element NP at 0x2b1da19c140>, <Element VP at 0x2b1da19ea00>, <Element PUNCT-PERIOD at 0x2b1da19ee40>]\n",
      "['PP', 'NP', 'VP']\n",
      "[<Element NP at 0x2b1da125a40>, <Element VP at 0x2b1da125cc0>, <Element PUNCT-PERIOD at 0x2b1da1271c0>]\n",
      "['NP', 'VP']\n",
      "[<Element SBAR at 0x2b1da125540>, <Element PUNCT-COMMA at 0x2b1da125940>, <Element NP at 0x2b1da126800>, <Element VP at 0x2b1da126280>, <Element PUNCT-PERIOD at 0x2b1da126b00>]\n",
      "['SBAR', 'NP', 'VP']\n",
      "[<Element S at 0x2b1da126dc0>, <Element PUNCT-COLON at 0x2b1da124700>, <Element CC at 0x2b1da127940>, <Element S at 0x2b1da124380>, <Element PUNCT-PERIOD at 0x2b1da1276c0>]\n",
      "['S', 'CC', 'S']\n",
      "[<Element S at 0x2b1da126440>, <Element PUNCT-COLON at 0x2b1da124940>, <Element CC at 0x2b1da126180>, <Element S at 0x2b1da126e00>, <Element PUNCT-PERIOD at 0x2b1dc14ed80>]\n",
      "['S', 'CC', 'S']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cedch\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\distributions\\distribution.py:45: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
      "  warnings.warn(f'{self.__class__} does not define `arg_constraints`. ' +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Element NP at 0x2b1da1da1c0>, <Element PUNCT-COLON at 0x2b1d5acab00>, <Element S at 0x2b1d5aa41c0>, <Element PUNCT-PERIOD at 0x2b1de9e2dc0>]\n",
      "['NP', 'S']\n",
      "[<Element SBAR at 0x2b1b5de2cc0>, <Element PUNCT-COMMA at 0x2b1b5de1200>, <Element NP at 0x2b1de2dfc80>, <Element VP at 0x2b1daba4c00>, <Element PUNCT-PERIOD at 0x2b1daba4e00>]\n",
      "['SBAR', 'NP', 'VP']\n",
      "[<Element PP at 0x2b1da972040>, <Element NP at 0x2b1da972300>, <Element VP at 0x2b1da971d40>, <Element PUNCT-PERIOD at 0x2b1da972f40>]\n",
      "['PP', 'NP', 'VP']\n",
      "[<Element NP at 0x2b1dc15c3c0>, <Element PUNCT-COLON at 0x2b1d5aa5dc0>, <Element SBAR at 0x2b1d5aa6780>, <Element PUNCT-PERIOD at 0x2b1d5aa7080>]\n",
      "['NP', 'SBAR']\n",
      "Error Log:\n",
      "\n",
      "                                              1789_Washington_Inaugural_Address  \\\n",
      "date                                                                 1789-04-30   \n",
      "pres_name                                                     George Washington   \n",
      "byline                          1st President of the United States: 1789 ‐ 1797   \n",
      "title                                                         Inaugural Address   \n",
      "benepar_analysis_time                                                       0.0   \n",
      "spacy_analysis_time                                                        0.23   \n",
      "tree_edit_distance_time                                                    6.09   \n",
      "total_file_analysis_time                                                  11.67   \n",
      "num_tokens                                                                 1546   \n",
      "num_sentences                                                                23   \n",
      "avg_tree_edit_dist_adjacent                                              138.23   \n",
      "avg_node_depth                                                            12.68   \n",
      "max_node_depth                                                               50   \n",
      "avg_node_clause_depth                                                      2.46   \n",
      "max_node_clause_depth                                                         8   \n",
      "avg_clause_length                                                         55.32   \n",
      "clauses_per_sent                                                           6.26   \n",
      "sbars_per_sent                                                              3.3   \n",
      "pronouns_per_sent                                                           4.7   \n",
      "pronouns_per_clause                                                        0.75   \n",
      "pronoun_prop_of_leaf_nps                                                   0.31   \n",
      "avg_num_np_modifiers                                                        5.1   \n",
      "loose_parataxis_per_sent                                                   1.48   \n",
      "root_parataxis_per_sent_strict                                              1.0   \n",
      "root_parataxis_per_sent_loose                                              1.17   \n",
      "num_unk                                                                       2   \n",
      "num_words                                                                  1434   \n",
      "avg_dependency_distance                                                    3.56   \n",
      "max_dependency_distance                                                      82   \n",
      "avg_sentence_length_by_tok                                                67.22   \n",
      "avg_sentence_length_by_word                                               62.35   \n",
      "avg_words_before_root                                                     13.48   \n",
      "num_uniq_words                                                              593   \n",
      "proportion_uniq                                                            0.42   \n",
      "stop_words_per_clause                                                      5.92   \n",
      "stop_words_per_sentence                                                   37.09   \n",
      "avg_aoa_max                                                                 6.2   \n",
      "avg_aoa_uniq_max                                                           8.12   \n",
      "avg_stopless_aoa_max                                                       8.62   \n",
      "avg_stopless_aoa_uniq_max                                                  8.84   \n",
      "avg_word_freq                                                           6791.01   \n",
      "avg_word_freq_uniq                                                        835.1   \n",
      "avg_word_freq_stopless                                                    94.66   \n",
      "avg_word_freq_stopless_uniq                                               76.64   \n",
      "\n",
      "                                              1793_Washington_Inaugural_Address  \n",
      "date                                                                 1793-03-04  \n",
      "pres_name                                                     George Washington  \n",
      "byline                          1st President of the United States: 1789 ‐ 1797  \n",
      "title                                                         Inaugural Address  \n",
      "benepar_analysis_time                                                       0.0  \n",
      "spacy_analysis_time                                                        0.15  \n",
      "tree_edit_distance_time                                                    0.14  \n",
      "total_file_analysis_time                                                   0.72  \n",
      "num_tokens                                                                  147  \n",
      "num_sentences                                                                 4  \n",
      "avg_tree_edit_dist_adjacent                                                79.0  \n",
      "avg_node_depth                                                             9.06  \n",
      "max_node_depth                                                               23  \n",
      "avg_node_clause_depth                                                      1.66  \n",
      "max_node_clause_depth                                                         4  \n",
      "avg_clause_length                                                          30.8  \n",
      "clauses_per_sent                                                           3.75  \n",
      "sbars_per_sent                                                             2.25  \n",
      "pronouns_per_sent                                                          3.25  \n",
      "pronouns_per_clause                                                        0.87  \n",
      "pronoun_prop_of_leaf_nps                                                   0.35  \n",
      "avg_num_np_modifiers                                                       4.11  \n",
      "loose_parataxis_per_sent                                                   1.25  \n",
      "root_parataxis_per_sent_strict                                              1.0  \n",
      "root_parataxis_per_sent_loose                                               1.0  \n",
      "num_unk                                                                       0  \n",
      "num_words                                                                   135  \n",
      "avg_dependency_distance                                                    3.07  \n",
      "max_dependency_distance                                                      46  \n",
      "avg_sentence_length_by_tok                                                36.75  \n",
      "avg_sentence_length_by_word                                               33.75  \n",
      "avg_words_before_root                                                      16.5  \n",
      "num_uniq_words                                                               90  \n",
      "proportion_uniq                                                            0.67  \n",
      "stop_words_per_clause                                                      5.27  \n",
      "stop_words_per_sentence                                                   19.75  \n",
      "avg_aoa_max                                                                6.05  \n",
      "avg_aoa_uniq_max                                                           6.92  \n",
      "avg_stopless_aoa_max                                                       8.58  \n",
      "avg_stopless_aoa_uniq_max                                                  8.52  \n",
      "avg_word_freq                                                           8143.49  \n",
      "avg_word_freq_uniq                                                      2832.02  \n",
      "avg_word_freq_stopless                                                     78.1  \n",
      "avg_word_freq_stopless_uniq                                                75.3  \n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame([])\n",
    "\n",
    "# TED Mode\n",
    "#  - 'combinations': Averages the TED of every pair of sentences in a doc\n",
    "#  - 'adjacent': Averages the TED of a sentence and the next sentence in a doc\n",
    "ted_mode = 'adjacent'\n",
    "# AoA Mode: If word is not in AoA list and multiple lemma matches exist\n",
    "#  - 'max': Chooses max of lemma matches\n",
    "#  - 'avg': Averages lemma matches\n",
    "aoa_mode = 'max'\n",
    "log = \"\"\n",
    "\n",
    "for i, file in enumerate(os.scandir('data/text_jsons/')):\n",
    "    if i == 2: break\n",
    "    \n",
    "    file_time = time.perf_counter()\n",
    "    file_name = re.sub('\\.json$', '', file.name)\n",
    "    with open(file, encoding='utf-8') as f:\n",
    "        metadata = json.load(f)\n",
    "    text = metadata['text']\n",
    "    text = text.replace('\\n', ' ').strip()\n",
    "    text = re.sub('\\s{2,}', ' ', text)\n",
    "    \n",
    "    try:\n",
    "        doc = nlp(text)\n",
    "    except ValueError:\n",
    "        error_text = f\"ValueError in '{file_name}'. Likely exists too long sentence. Skipping.\"\n",
    "        log += error_text + '\\n'\n",
    "        continue\n",
    "    except:\n",
    "        error_text = f\"Some other error occured in '{file_name}'. Skipping.\"\n",
    "        log += error_text + '\\n'\n",
    "        continue\n",
    "\n",
    "    sents = list(doc.sents)\n",
    "\n",
    "    # Doc-level\n",
    "    num_tokens = len(doc)\n",
    "    num_sents = len(sents)\n",
    "    ted_sum = 0\n",
    "\n",
    "    features_lst = ['num_clauses', 'num_sbar', 'num_unk', 'depth_sum', 'max_depth', \n",
    "        'max_clause_depth', 'clause_depth_sum', 'pronoun_sum', 'num_leaf_nps', 'num_nps', \n",
    "        'np_leaf_sum', 'clause_length_sum', 'paratactic_sum', 'root_parataxis_strict', \n",
    "        'root_parataxis_loose', 'dep_dist_sum', 'num_words', 'words_before_root_sum', 'uniq_words', \n",
    "        'num_words_no_nums', 'num_stop_words', 'max_dep_dist', 'aoa_sum', 'aoa_count', \n",
    "        'aoa_stopless_sum', 'aoa_stopless_count', 'aoa_uniq_sum', 'aoa_uniq_count',\n",
    "        'aoa_stopless_uniq_sum', 'aoa_stopless_uniq_count', 'wf_sum', 'wf_count', \n",
    "        'wf_stopless_sum', 'wf_stopless_count', 'wf_uniq_sum', 'wf_uniq_count',\n",
    "        'wf_stopless_uniq_sum', 'wf_stopless_uniq_count']\n",
    "    features = Counter({x : 0 for x in features_lst})\n",
    "    uniq_words = set()\n",
    "\n",
    "    for sent in sents:\n",
    "        benepar_time = time.perf_counter()\n",
    "        b_features, b_max_features = benepar_analysis(sent)\n",
    "        features.update(b_features)\n",
    "        for feature in b_max_features:\n",
    "            features[feature] = max(features.get(feature), b_max_features[feature])\n",
    "        benepar_time = time.perf_counter() - benepar_time\n",
    "\n",
    "        spacy_time = time.perf_counter()\n",
    "        s_features, s_max_features, sent_vocab = spacy_analysis(sent, uniq_words)\n",
    "        features.update(s_features)\n",
    "        for feature in s_max_features:\n",
    "            features[feature] = max(features.get(feature), s_max_features[feature])\n",
    "        uniq_words.update(sent_vocab)\n",
    "        spacy_time = time.perf_counter() - spacy_time\n",
    "\n",
    "    # TREE EDIT DISTANCE\n",
    "    ted_time = time.perf_counter()\n",
    "    ted_sum = 0\n",
    "    if ted_mode == 'adjacent':    \n",
    "        for i in range(num_sents - 1):\n",
    "            ted_sum += ted_analysis(sents[i], sents[i + 1])\n",
    "        ted_avg = ted_sum / (num_sents - 1)\n",
    "    elif ted_mode == 'combinations':\n",
    "        for sent1, sent2 in combinations(sents, 2):\n",
    "            ted_sum += ted_analysis(sent1, sent2)\n",
    "        ted_avg = ted_sum / comb(num_sents, 2)\n",
    "    else:\n",
    "        print('Invalid ted_mode:', ted_mode)\n",
    "        ted_avg = -1\n",
    "    ted_time = time.perf_counter() - ted_time\n",
    "\n",
    "    summary = {\n",
    "        # File-level\n",
    "        'date' : metadata['date'],\n",
    "        'pres_name' : metadata['pres_name'],\n",
    "        'byline' : metadata['byline'],\n",
    "        'title' : metadata['title'],\n",
    "\n",
    "        # Performance time\n",
    "        'benepar_analysis_time' : benepar_time,\n",
    "        'spacy_analysis_time' : spacy_time,\n",
    "        'tree_edit_distance_time' : ted_time,\n",
    "        'total_file_analysis_time' : time.perf_counter() - file_time,\n",
    "\n",
    "        # Doc-level\n",
    "        'num_tokens' : num_tokens,\n",
    "        'num_sentences' : num_sents, \n",
    "        # 'avg_ted_adj' : ted_avg_adj,\n",
    "        # 'avg_ted_comb' : ted_avg_comb,\n",
    "        f'avg_tree_edit_dist_{ted_mode}' : ted_avg,\n",
    "\n",
    "        # Benepar\n",
    "        'avg_node_depth' : features['depth_sum'] / num_tokens, \n",
    "        'max_node_depth' : features['max_depth'], # Equivalent to tree height\n",
    "        'avg_node_clause_depth' : features['clause_depth_sum'] / num_tokens,\n",
    "        'max_node_clause_depth' : features['max_clause_depth'],\n",
    "        'avg_clause_length' : features['clause_length_sum'] / features['num_clauses'],\n",
    "        'clauses_per_sent' : features['num_clauses'] / num_sents, \n",
    "        'sbars_per_sent' : features['num_sbar'] / num_sents,\n",
    "        'pronouns_per_sent' : features['pronoun_sum'] / num_sents,\n",
    "        'pronouns_per_clause' : features['pronoun_sum'] / features['num_clauses'],\n",
    "        'pronoun_prop_of_leaf_nps' : features['pronoun_sum'] / features['num_leaf_nps'],\n",
    "        'avg_num_np_modifiers' : features['np_leaf_sum'] / features['num_nps'],\n",
    "        'loose_parataxis_per_sent' : features['paratactic_sum'] / num_sents,\n",
    "        'root_parataxis_per_sent_strict' : features['root_parataxis_strict'] / num_sents,\n",
    "        'root_parataxis_per_sent_loose' : features['root_parataxis_loose'] / num_sents,\n",
    "        'num_unk' : features['num_unk'],\n",
    "\n",
    "        # spaCy\n",
    "        'num_words' : features['num_words'],\n",
    "        'avg_dependency_distance' : features['dep_dist_sum'] / features['num_words'],\n",
    "        'max_dependency_distance' : features['max_dep_dist'],\n",
    "        'avg_sentence_length_by_tok' : num_tokens / num_sents, \n",
    "        'avg_sentence_length_by_word' : features['num_words'] / num_sents,\n",
    "        'avg_words_before_root' : features['words_before_root_sum'] / num_sents,\n",
    "        'num_uniq_words' : len(uniq_words), \n",
    "        'proportion_uniq' : len(uniq_words) / features['num_words_no_nums'],\n",
    "        'stop_words_per_clause' : features['num_stop_words'] / features['num_clauses'],\n",
    "        'stop_words_per_sentence' : features['num_stop_words'] / num_sents,\n",
    "        f'avg_aoa_{aoa_mode}' : features['aoa_sum'] / features['aoa_count'],\n",
    "        f'avg_aoa_uniq_{aoa_mode}' : features['aoa_uniq_sum'] / features['aoa_uniq_count'],\n",
    "        f'avg_stopless_aoa_{aoa_mode}' : features['aoa_stopless_sum'] / features['aoa_stopless_count'],\n",
    "        f'avg_stopless_aoa_uniq_{aoa_mode}' : features['aoa_stopless_uniq_sum'] / features['aoa_stopless_uniq_count'],\n",
    "        'avg_word_freq' : features['wf_sum'] / features['wf_count'],\n",
    "        'avg_word_freq_uniq' : features['wf_uniq_sum'] / features['wf_uniq_count'],\n",
    "        'avg_word_freq_stopless' : features['wf_stopless_sum'] / features['wf_stopless_count'],\n",
    "        'avg_word_freq_stopless_uniq' : features['wf_stopless_uniq_sum'] / features['wf_stopless_uniq_count'],\n",
    "    }      \n",
    "    \n",
    "    results[file_name] = summary\n",
    "    \n",
    "print(\"Error Log:\")\n",
    "print(log)\n",
    "pd.set_option('display.precision', 2)\n",
    "print(results)\n",
    "results.to_csv(f\"results/{datetime.now().strftime('%m-%d-%Y_%H-%M')}.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.798829799226471   0.13868336305891654 0.03236415372895239]\n",
      "0.96987731601434\n",
      "                                1         2         3\n",
      "avg_word_freq_stopless       0.84  1.72e-01 -4.53e-01\n",
      "avg_word_freq_stopless_uniq  0.43  4.56e-02  8.78e-01\n",
      "avg_tree_edit_dist_adjacent -0.18  2.06e-01  6.46e-02\n",
      "max_dependency_distance     -0.17  8.83e-01 -2.74e-02\n",
      "avg_sentence_length_by_tok  -0.10  1.34e-02 -1.67e-03\n",
      "avg_sentence_length_by_word -0.10  3.90e-03 -2.90e-03\n",
      "avg_clause_length           -0.08  7.60e-03 -2.08e-02\n",
      "stop_words_per_sentence     -0.06 -2.67e-02 -4.49e-03\n",
      "max_node_depth              -0.06  1.76e-01  6.01e-02\n",
      "avg_words_before_root       -0.05 -3.63e-02 -1.58e-02\n",
      "avg_node_depth              -0.03 -5.57e-02 -1.47e-02\n",
      "stop_words_per_clause       -0.03 -7.92e-02 -2.89e-02\n",
      "avg_num_np_modifiers        -0.03 -6.82e-02 -1.99e-02\n",
      "clauses_per_sent            -0.03 -7.66e-02 -2.25e-02\n",
      "avg_word_freq               -0.03 -2.72e-02  2.35e-03\n",
      "avg_stopless_aoa_max        -0.03 -8.39e-02 -2.90e-02\n",
      "pronouns_per_sent           -0.03 -7.72e-02 -1.83e-02\n",
      "sbars_per_sent              -0.02 -8.05e-02 -2.43e-02\n",
      "avg_stopless_aoa_uniq_max   -0.02 -8.37e-02 -3.27e-02\n",
      "avg_aoa_uniq_max            -0.02 -8.36e-02 -3.66e-02\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "\n",
    "# print(results['num_tokens':])\n",
    "np.set_printoptions(precision=100)\n",
    "\n",
    "results_data = results['num_tokens':].drop(['num_tokens', 'num_words', \n",
    "    'num_sentences', 'num_unk', 'num_uniq_words', 'avg_word_freq_uniq'])\n",
    "# results_data = results['num_tokens':]\n",
    "results_std = StandardScaler().fit_transform(results_data).T\n",
    "# results_std = normalize(results_data).T\n",
    "\n",
    "# print([np.max(i)-np.min(i) for i in results_std])\n",
    "\n",
    "# print(results_std.shape)\n",
    "# fa = FactorAnalysis(n_components=3)\n",
    "# f = fa.fit(results_std).components_.T\n",
    "# # print(np.sum(f.T[0]))\n",
    "# # print(f)\n",
    "\n",
    "# # print(fa.fit_transform(results_std.T))\n",
    "\n",
    "# faf = pd.DataFrame(f, index=list(results_data.index), columns=['1', '2', '3'])\n",
    "# print(faf.nlargest(10, ['1']))\n",
    "\n",
    "\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=3).fit(results_std)\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(sum(pca.explained_variance_ratio_))\n",
    "# print(pca.components_.shape)\n",
    "# print(pca.components_.T)\n",
    "\n",
    "comps = pd.DataFrame(pca.components_.T, index=list(results_data.index), columns=['1', '2', '3'])\n",
    "m = comps.abs().nlargest(20, ['1']).index\n",
    "print(comps.loc[m])\n",
    "\n",
    "print('num_tokens' in comps.index)\n",
    "\n",
    "# print('-'*100)\n",
    "\n",
    "# cov = np.cov(results_std.T)\n",
    "\n",
    "# evals, evecs = np.linalg.eig(cov)\n",
    "\n",
    "# print(evecs)\n",
    "# print(evecs.shape)\n",
    "\n",
    "# explained_variances = []\n",
    "# for i in range(len(evals)):\n",
    "#     explained_variances.append(evals[i] / np.sum(evals))\n",
    " \n",
    "# print(np.sum(explained_variances), '\\n', explained_variances)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- vocab measures\n",
    "    - measures of polysemy\n",
    "- weight ted differently --> should adding/removing be weighted less because it simply indicates a different length sentence?\n",
    "    - how to remove determiners?\n",
    "- make scraper more robust\n",
    "- unit testing for parataxis measures\n",
    "\n",
    "- word not in AoA list \n",
    "    - opt 1: average lemmas\n",
    "    - opt 2: take max\n",
    "\n",
    "- cosine similarity\n",
    "    - Something at the end???\n",
    "\n",
    "- PCA and RFE\n",
    "- maybe paratactic should go back to the regular clause tags because we don't want S:SBAR cuz they're related\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions\n",
    "- Workaround for the spacy max length? <code>ValueError: Sentence of length 965 (in sub-word tokens) exceeds the maximum supported length of 512</code>\n",
    "- How to compare syntax trees better?\n",
    "    - Should I do TED weighting?\n",
    "- Visually displaying / analyzing the data\n",
    "- Code not running on harris..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- View: https://scholarworks.gsu.edu/cgi/viewcontent.cgi?article=1035&context=alesl_diss\n",
    "- View: http://cohmetrix.memphis.edu/cohmetrixhome/documentation_indices.html#Complexity\n",
    "- MORE RESEARCH NEEDED: compare tree similarity (SEARCH TREE EDIT DISTANCE) of sentences in doc, pq-gram distance\n",
    "(CITING APTED: https://pypi.org/project/apted/#description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tools'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtools\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmisc_tools\u001b[39;00m \u001b[39mimport\u001b[39;00m sexp_to_xml\n\u001b[0;32m      2\u001b[0m p_sum \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m      3\u001b[0m p_suml \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tools'"
     ]
    }
   ],
   "source": [
    "from tools.misc_tools import sexp_to_xml\n",
    "p_sum = 0\n",
    "p_suml = 0\n",
    "d = nlp(\"This oath I am now about to take, and in your presence: That if it shall be found during my administration of the Government I have in any instance violated willingly or knowingly the injunctions thereof, I may (besides incurring constitutional punishment) be subject to the upbraidings of all who are now witnesses of the present solemn ceremony.\")\n",
    "d = nlp(\"this that he will go: that i am clear\")\n",
    "# d=nlp(\"to go: that i am clear\")\n",
    "# returns p_sum 2, p_suml 4\n",
    "s = list(d.sents)[0]\n",
    "print(s)\n",
    "xml = sexp_to_xml(s._.parse_string)\n",
    "print(s._.parse_string)\n",
    "print(xml)\n",
    "root = etree.fromstring(xml) \n",
    "tree = etree.ElementTree(root)\n",
    "print(type(root))\n",
    "print(root.getchildren())\n",
    "print(root)\n",
    "\n",
    "p_sum += find_parataxis_strict(root)\n",
    "for e in root.iter():\n",
    "    tag = e.tag\n",
    "    if tag in clause_tags:\n",
    "        p_suml += sum(int(bool(c.tag in parataxis_clause_tags)) for c in e.getchildren())\n",
    "\n",
    "\n",
    "print(p_sum, p_suml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This oath I am now about to take, and in your presence: That if it shall be found during my administration of the Government I have in any instance violated willingly or knowingly the injunctions thereof, I may (besides incurring constitutional punishment) be subject to the upbraidings of all who are now witnesses of the present solemn ceremony.\n",
      "<class 'lxml.etree._Element'>\n",
      "[<Element SBAR at 0x1ef11a6d280>, <Element COMMA at 0x1ef11833100>, <Element NP at 0x1ef11833d00>, <Element VP at 0x1ef118314c0>]\n",
      "<Element S at 0x1ef11a04680>\n",
      "When it was first perceived , in early times , that no middle course for America remained between unlimited submission to a foreign legislature and a total independence of its claims , men of reflection were less apprehensive of danger from the formidable power of fleets and armies they must determine to resist than from those contests and dissensions which would certainly arise concerning the forms of government to be instituted over the whole and over the parts of this extensive country\n",
      "When it was first perceived , in early times , that no middle course for America remained between unlimited submission to a foreign legislature and a total independence of its claims , men of reflection were less apprehensive of danger from the formidable power of fleets and armies they must determine to resist than from those contests and dissensions which would certainly arise concerning the forms of government to be instituted over the whole and over the parts of this extensive country\n",
      "When it was first perceived , in early times , that no middle course for America remained between unlimited submission to a foreign legislature and a total independence of its claims , men of reflection were less apprehensive of danger from the formidable power of fleets and armies they must determine to resist than from those contests and dissensions which would certainly arise concerning the forms of government to be instituted over the whole and over the parts of this extensive country\n",
      "When it was first perceived , in early times , that no middle course for America remained between unlimited submission to a foreign legislature and a total independence of its claims , men of reflection were less apprehensive of danger from the formidable power of fleets and armies they must determine to resist than from those contests and dissensions which would certainly arise concerning the forms of government to be instituted over the whole and over the parts of this extensive country\n",
      "When it was first perceived , in early times , that no middle course for America remained between unlimited submission to a foreign legislature and a total independence of its claims , men of reflection were less apprehensive of danger from the formidable power of fleets and armies they must determine to resist than from those contests and dissensions which would certainly arise concerning the forms of government to be instituted over the whole and over the parts of this extensive country\n",
      "When it was first perceived , in early times , that no middle course for America remained between unlimited submission to a foreign legislature and a total independence of its claims , men of reflection were less apprehensive of danger from the formidable power of fleets and armies they must determine to resist than from those contests and dissensions which would certainly arise concerning the forms of government to be instituted over the whole and over the parts of this extensive country\n",
      "When it was first perceived , in early times , that no middle course for America remained between unlimited submission to a foreign legislature and a total independence of its claims , men of reflection were less apprehensive of danger from the formidable power of fleets and armies they must determine to resist than from those contests and dissensions which would certainly arise concerning the forms of government to be instituted over the whole and over the parts of this extensive country\n",
      "0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cedch\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\distributions\\distribution.py:45: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
      "  warnings.warn(f'{self.__class__} does not define `arg_constraints`. ' +\n"
     ]
    }
   ],
   "source": [
    "\n",
    "p_sum = 0\n",
    "p_suml = 0\n",
    "\n",
    "d = nlp(\"This oath I am now about to take, and in your presence: That if it shall be found during my administration of the Government I have in any instance violated willingly or knowingly the injunctions thereof, I may (besides incurring constitutional punishment) be subject to the upbraidings of all who are now witnesses of the present solemn ceremony.\")\n",
    "# d=nlp(\"to go: that i am clear\")\n",
    "# returns p_sum 2, p_suml 4\n",
    "s = list(d.sents)[0]\n",
    "print(s)\n",
    "# xml = sexp_to_xml(s._.parse_string)\n",
    "# print(s._.parse_string)\n",
    "# print(xml)\n",
    "xml = \"<S><SBAR><WHADVP><WRB>When</WRB></WHADVP><S><NP><PRP>it</PRP></NP><VP><VBD>was</VBD><ADVP><RB>first</RB></ADVP><VP><VBN>perceived</VBN><COMMA>,</COMMA><PP><IN>in</IN><NP><JJ>early</JJ><NNS>times</NNS></NP></PP><COMMA>,</COMMA><SBAR><IN>that</IN><S><NP><NP><DT>no</DT><JJ>middle</JJ><NN>course</NN></NP><PP><IN>for</IN><NP><NNP>America</NNP></NP></PP></NP><VP><VBD>remained</VBD><PP><IN>between</IN><NP><NP><NP><JJ>unlimited</JJ><NN>submission</NN></NP><PP><TO>to</TO><NP><DT>a</DT><JJ>foreign</JJ><NN>legislature</NN></NP></PP></NP><CC>and</CC><NP><NP><DT>a</DT><JJ>total</JJ><NN>independence</NN></NP><PP><IN>of</IN><NP><PRPS>its</PRPS><NNS>claims</NNS></NP></PP></NP></NP></PP></VP></S></SBAR></VP></VP></S></SBAR><COMMA>,</COMMA><NP><NP><NNS>men</NNS></NP><PP><IN>of</IN><NP><NN>reflection</NN></NP></PP></NP><VP><VBD>were</VBD><ADJP><ADJP><RBR>less</RBR><JJ>apprehensive</JJ><PP><IN>of</IN><NP><NP><NN>danger</NN></NP><PP><IN>from</IN><NP><NP><DT>the</DT><JJ>formidable</JJ><NN>power</NN></NP><PP><PP><IN>of</IN><NP><NNS>fleets</NNS><CC>and</CC><NNS>armies</NNS></NP></PP><SBAR><S><NP><PRP>they</PRP></NP><VP><MD>must</MD><VP><VB>determine</VB><S><VP><TO>to</TO><VP><VB>resist</VB></VP></VP></S></VP></VP></S></SBAR></PP></NP></PP></NP></PP></ADJP><PP><IN>than</IN><PP><IN>from</IN><NP><NP><DT>those</DT><NNS>contests</NNS><CC>and</CC><NNS>dissensions</NNS></NP><SBAR><WHNP><WDT>which</WDT></WHNP><S><VP><MD>would</MD><ADVP><RB>certainly</RB></ADVP><VP><VB>arise</VB><PP><VBG>concerning</VBG><NP><NP><DT>the</DT><NNS>forms</NNS></NP><PP><IN>of</IN><NP><NN>government</NN></NP></PP><SBAR><S><VP><TO>to</TO><VP><VB>be</VB><VP><VBN>instituted</VBN><PP><PP><IN>over</IN><NP><DT>the</DT><NN>whole</NN></NP></PP><CC>and</CC><PP><IN>over</IN><NP><NP><DT>the</DT><NNS>parts</NNS></NP><PP><IN>of</IN><NP><DT>this</DT><JJ>extensive</JJ><NN>country</NN></NP></PP></NP></PP></PP></VP></VP></VP></S></SBAR></NP></PP></VP></VP></S></SBAR></NP></PP></PP></ADJP></VP></S>\"\n",
    "root = etree.fromstring(xml) \n",
    "tree = etree.ElementTree(root)\n",
    "print(type(root))\n",
    "print(root.getchildren())\n",
    "print(root)\n",
    "\n",
    "CLAUSE_TAGS = ['S', 'SBARQ', 'SINV'] # Not included: 'SQ', 'SBAR'\n",
    "\n",
    "for e in root.iter():\n",
    "    tag = e.tag\n",
    "    if tag in CLAUSE_TAGS:\n",
    "        print(' '.join([x for x in root.itertext()]))\n",
    "\n",
    "\n",
    "print(p_sum, p_suml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install benepar\n",
    "! pip install spacy\n",
    "! pip install apted\n",
    "! pip install bs4\n",
    "! python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'15.5': 100, '16.5': 100})\n",
      "Counter({'15.5': 200, '14.5': 100, '16.5': 100})\n"
     ]
    }
   ],
   "source": [
    "a=[(\"13.5\",100)]\n",
    "b=[(\"14.5\",100), (\"15.5\", 100)]\n",
    "c=[(\"15.5\",100), (\"16.5\", 100)]\n",
    "b={\"14.5\":100, \"15.5\": 100}\n",
    "c={\"15.5\":100, \"16.5\": 100}\n",
    "input=[b, c]\n",
    "\n",
    "from collections import Counter\n",
    "print(Counter(c))\n",
    "print(sum(\n",
    "    (Counter(x) for x in input),\n",
    "    Counter()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'results/complete_12-07.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [2], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m normalize\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdecomposition\u001b[39;00m \u001b[39mimport\u001b[39;00m FactorAnalysis\n\u001b[1;32m----> 8\u001b[0m results \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39mresults/complete_12-07.csv\u001b[39;49m\u001b[39m'\u001b[39;49m, index_col\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m      9\u001b[0m \u001b[39m# print(results['num_tokens':])\u001b[39;00m\n\u001b[0;32m     11\u001b[0m results_data \u001b[39m=\u001b[39m results[\u001b[39m'\u001b[39m\u001b[39mnum_tokens\u001b[39m\u001b[39m'\u001b[39m:]\u001b[39m.\u001b[39mdrop([\u001b[39m'\u001b[39m\u001b[39mnum_tokens\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mnum_words\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[0;32m     12\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mnum_sentences\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mnum_unk\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mnum_uniq_words\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mavg_word_freq_uniq\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\util\\_decorators.py:317\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    312\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    313\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    314\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    315\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(inspect\u001b[39m.\u001b[39mcurrentframe()),\n\u001b[0;32m    316\u001b[0m     )\n\u001b[1;32m--> 317\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:1729\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1727\u001b[0m     is_text \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1728\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1729\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1730\u001b[0m     f,\n\u001b[0;32m   1731\u001b[0m     mode,\n\u001b[0;32m   1732\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1733\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1734\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1735\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1736\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1737\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1738\u001b[0m )\n\u001b[0;32m   1739\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1740\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\common.py:857\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    852\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    853\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    855\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    856\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    858\u001b[0m             handle,\n\u001b[0;32m    859\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    860\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    861\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    862\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    863\u001b[0m         )\n\u001b[0;32m    864\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    865\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    866\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'results/complete_12-07.csv'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "\n",
    "results = pd.read_csv('results/complete_12-07.csv', index_col=0)\n",
    "# print(results['num_tokens':])\n",
    "\n",
    "# results_data = results['num_tokens':]\n",
    "results_std = StandardScaler().fit_transform(results_data).T\n",
    "# results_std = normalize(results_data).T\n",
    "\n",
    "# print([np.max(i)-np.min(i) for i in results_std])\n",
    "\n",
    "# print(results_std.shape)\n",
    "# fa = FactorAnalysis(n_components=3)\n",
    "# f = fa.fit(results_std).components_.T\n",
    "# # print(np.sum(f.T[0]))\n",
    "# # print(f)\n",
    "\n",
    "# # print(fa.fit_transform(results_std.T))\n",
    "\n",
    "# faf = pd.DataFrame(f, index=list(results_data.index), columns=['1', '2', '3'])\n",
    "# print(faf.nlargest(10, ['1']))\n",
    "\n",
    "\n",
    "\n",
    "# # PCA\n",
    "# pca = PCA(n_components=3).fit(results_std)\n",
    "# print(pca.explained_variance_ratio_)\n",
    "# print(sum(pca.explained_variance_ratio_))\n",
    "# # print(pca.components_.shape)\n",
    "# # print(pca.components_.T)\n",
    "\n",
    "# comps = pd.DataFrame(pca.components_.T, index=list(results_data.index), columns=['1', '2', '3'])\n",
    "# m = comps.abs().nlargest(20, ['1']).index\n",
    "# print(comps.loc[m])\n",
    "\n",
    "# print('num_tokens' in comps.index)\n",
    "\n",
    "# # print('-'*100)\n",
    "\n",
    "# cov = np.cov(results_std.T)\n",
    "\n",
    "# evals, evecs = np.linalg.eig(cov)\n",
    "# print(evecs)\n",
    "\n",
    "\n",
    "\n",
    "# print(evecs.shape)\n",
    "\n",
    "# explained_variances = []\n",
    "# for i in range(len(evals)):\n",
    "#     explained_variances.append(evals[i] / np.sum(evals))\n",
    " \n",
    "# print(np.sum(explained_variances), '\\n', explained_variances)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(results)\n",
    "\n",
    "print(results['num_sentences'])\n",
    "\n",
    "# sns.lmplot(data=results.T, x='date', y='num_words')\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9cc068bd1879d5f924fce910ac6f0b7ba67f436e44a09dbef81e8920d2908e28"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
